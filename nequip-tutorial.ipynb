{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NequIP Tutorial \n",
    "This is an implementation of an NequIP energy and force prediction model in python. \n",
    "The trained model is deployed and integrated into LAMMPS MD engine to run an accelerated simulation on a single molecule.\n",
    "Questions? How does the integration works? How to setup LAMMPS simulation? How to determine performance (accuracy) of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float32)\n",
    "import numpy as np\n",
    "import logging\n",
    "import pprint\n",
    "from nequip.utils.config import Config\n",
    "#from nequip.train.trainer import Trainer\n",
    "from nequip.data import AtomicDataDict\n",
    "from nequip.data import AtomicData\n",
    "from nequip.data import dataset_from_config\n",
    "from ase.io import read, write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Visualize Molecule Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '*.zip': No such file or directory\n",
      "rm: cannot remove '*.npz': No such file or directory\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1020k  100 1020k    0     0   482k      0  0:00:02  0:00:02 --:--:--  482k\n",
      "Archive:  outfile.zip\n",
      "  inflating: toluene_ccsd_t-train.npz  \n",
      "   creating: __MACOSX/\n",
      "  inflating: __MACOSX/._toluene_ccsd_t-train.npz  \n",
      "  inflating: toluene_ccsd_t-test.npz  \n",
      "  inflating: __MACOSX/._toluene_ccsd_t-test.npz  \n",
      "/usr/bin/sh: 1: y: not found\n",
      "toluene_ccsd_t-test.npz  toluene_ccsd_t-train.npz\n"
     ]
    }
   ],
   "source": [
    "# Remove existing sample folders and download molecule dataset\n",
    "!rm -rf ./results/\n",
    "!rm -rf ./benchmark_data\n",
    "!rm *.zip\n",
    "!rm *.npz\n",
    "!mkdir benchmark_data\n",
    "!curl http://quantum-machine.org/gdml/data/npz/toluene_ccsd_t.zip -o outfile.zip\n",
    "!unzip outfile.zip\n",
    "!y | rm -rf __MACOSX outfile outfile.zip\n",
    "!mv toluene* ./benchmark_data\n",
    "!ls benchmark_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "from ase.io import read\n",
    "atoms = read('toluene.xyz', index=0)\n",
    "\n",
    "from ase.visualize import view\n",
    "view(atoms, viewer='x3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set torch geometric training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'name', 'F', 'theory', 'R', 'z', 'type', 'md5']\n",
      "(1000, 15, 3)\n"
     ]
    }
   ],
   "source": [
    "# from nequip.data.dataset import NpzDataset\n",
    "# dataset = NpzDataset('./tutorial_results/')\n",
    "# # logging.info(f\"Successfully loaded the data set of type {dataset}...\")\n",
    "npz_files = np.load('benchmark_data/toluene_ccsd_t-train.npz')\n",
    "npz_files['E'].shape\n",
    "npz_files['z']\n",
    "npz_files['name']\n",
    "print(npz_files.files)\n",
    "print(npz_files['R'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BesselBasis_trainable': True,\n",
      " 'PolynomialCutoff_p': 6,\n",
      " 'append': True,\n",
      " 'avg_num_neighbors': 'auto',\n",
      " 'batch_size': 5,\n",
      " 'chemical_symbols': ['H', 'C'],\n",
      " 'conv_to_output_hidden_irreps_out': '16x0e',\n",
      " 'dataset': 'npz',\n",
      " 'dataset_file_name': './benchmark_data/toluene_ccsd_t-train.npz',\n",
      " 'dataset_seed': 456,\n",
      " 'dataset_url': 'http://quantum-machine.org/gdml/data/npz/toluene_ccsd_t.zip',\n",
      " 'default_dtype': 'float32',\n",
      " 'early_stopping_lower_bounds': {'LR': 1e-05},\n",
      " 'early_stopping_patiences': {'validation_loss': 50},\n",
      " 'ema_decay': 0.99,\n",
      " 'ema_use_num_updates': True,\n",
      " 'feature_irreps_hidden': '16x0o + 16x0e + 16x1o + 16x1e',\n",
      " 'invariant_layers': 2,\n",
      " 'invariant_neurons': 64,\n",
      " 'irreps_edge_sh': '0e + 1o',\n",
      " 'key_mapping': {'E': 'total_energy',\n",
      "                 'F': 'forces',\n",
      "                 'R': 'pos',\n",
      "                 'z': 'atomic_numbers'},\n",
      " 'l_max': 2,\n",
      " 'learning_rate': 0.005,\n",
      " 'log_batch_freq': 10,\n",
      " 'log_epoch_freq': 1,\n",
      " 'loss_coeffs': {'forces': 1, 'total_energy': [1, 'PerAtomMSELoss']},\n",
      " 'lr_scheduler_factor': 0.5,\n",
      " 'lr_scheduler_name': 'ReduceLROnPlateau',\n",
      " 'lr_scheduler_patience': 100,\n",
      " 'max_epochs': 100000,\n",
      " 'metrics_components': [['forces', 'mae'],\n",
      "                        ['forces', 'rmse'],\n",
      "                        ['forces',\n",
      "                         'mae',\n",
      "                         {'PerSpecies': True, 'report_per_component': False}],\n",
      "                        ['forces',\n",
      "                         'rmse',\n",
      "                         {'PerSpecies': True, 'report_per_component': False}],\n",
      "                        ['total_energy', 'mae'],\n",
      "                        ['total_energy', 'mae', {'PerAtom': True}]],\n",
      " 'metrics_key': 'validation_loss',\n",
      " 'n_train': 100,\n",
      " 'n_val': 50,\n",
      " 'nonlinearity_gates': {'e': 'silu', 'o': 'tanh'},\n",
      " 'nonlinearity_scalars': {'e': 'silu', 'o': 'tanh'},\n",
      " 'nonlinearity_type': 'gate',\n",
      " 'npz_fixed_field_keys': ['atomic_numbers'],\n",
      " 'num_basis': 8,\n",
      " 'num_features': 32,\n",
      " 'num_layers': 4,\n",
      " 'num_types': 2,\n",
      " 'optimizer_amsgrad': True,\n",
      " 'optimizer_name': 'Adam',\n",
      " 'parity': True,\n",
      " 'per_species_rescale_scales': 'dataset_forces_rms',\n",
      " 'per_species_rescale_scales_trainable': False,\n",
      " 'per_species_rescale_shifts': 'dataset_per_atom_total_energy_mean',\n",
      " 'per_species_rescale_shifts_trainable': False,\n",
      " 'r_max': 4.0,\n",
      " 'report_init_validation': True,\n",
      " 'root': 'results/toluene',\n",
      " 'run_name': 'example-run-toluene',\n",
      " 'save_checkpoint_freq': -1,\n",
      " 'save_ema_checkpoint_freq': -1,\n",
      " 'seed': 123,\n",
      " 'shuffle': True,\n",
      " 'train_val_split': 'random',\n",
      " 'use_ema': True,\n",
      " 'use_sc': True,\n",
      " 'validation_batch_size': 10,\n",
      " 'verbose': 'info',\n",
      " 'wandb': True,\n",
      " 'wandb_project': 'toluene-example'}\n"
     ]
    }
   ],
   "source": [
    "# Load config file\n",
    "config = Config.from_file('./example.yaml')\n",
    "pprint.pprint(config.as_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'root': 'results/toluene',\n",
       " 'run_name': 'example-run-toluene',\n",
       " 'append': True,\n",
       " 'default_dtype': 'float32',\n",
       " 'r_max': 4.0,\n",
       " 'num_layers': 4,\n",
       " 'l_max': 2,\n",
       " 'parity': True,\n",
       " 'num_features': 32,\n",
       " 'nonlinearity_type': 'gate',\n",
       " 'nonlinearity_scalars': {'e': 'silu', 'o': 'tanh'},\n",
       " 'nonlinearity_gates': {'e': 'silu', 'o': 'tanh'},\n",
       " 'num_basis': 8,\n",
       " 'BesselBasis_trainable': True,\n",
       " 'PolynomialCutoff_p': 6,\n",
       " 'invariant_layers': 2,\n",
       " 'invariant_neurons': 64,\n",
       " 'avg_num_neighbors': 'auto',\n",
       " 'use_sc': True,\n",
       " 'dataset': 'npz',\n",
       " 'dataset_url': 'http://quantum-machine.org/gdml/data/npz/toluene_ccsd_t.zip',\n",
       " 'dataset_file_name': './benchmark_data/toluene_ccsd_t-train.npz',\n",
       " 'key_mapping': {'z': 'atomic_numbers',\n",
       "  'E': 'total_energy',\n",
       "  'F': 'forces',\n",
       "  'R': 'pos'},\n",
       " 'npz_fixed_field_keys': ['atomic_numbers'],\n",
       " 'chemical_symbols': ['H', 'C'],\n",
       " 'num_types': 2,\n",
       " 'feature_irreps_hidden': '16x0o + 16x0e + 16x1o + 16x1e',\n",
       " 'irreps_edge_sh': '0e + 1o',\n",
       " 'conv_to_output_hidden_irreps_out': '16x0e',\n",
       " 'wandb': True,\n",
       " 'wandb_project': 'toluene-example',\n",
       " 'early_stopping_patiences': {'validation_loss': 50},\n",
       " 'early_stopping_lower_bounds': {'LR': 1e-05},\n",
       " 'optimizer_amsgrad': True,\n",
       " 'lr_scheduler_patience': 100,\n",
       " 'lr_scheduler_factor': 0.5,\n",
       " 'per_species_rescale_shifts_trainable': False,\n",
       " 'per_species_rescale_scales_trainable': False,\n",
       " 'per_species_rescale_shifts': 'dataset_per_atom_total_energy_mean',\n",
       " 'per_species_rescale_scales': 'dataset_forces_rms'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate Trainer object trainer with a config file. The trainer handles training as well as call back functions for logging, model saving, and early stopping.\n",
    "from nequip.train.trainer_wandb import TrainerWandB as Trainer\n",
    "trainer = Trainer(model=None, **dict(config))\n",
    "trainer.kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully loaded the data set of type NpzDataset(1000)...\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_from_config(config, prefix='dataset')\n",
    "logging.info(f\"Successfully loaded the data set of type {dataset}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['total_energy', 'forces', 'pos'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.set_dataset(dataset)\n",
    "trainer.dataset_train.get_data()[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forces standard deviation: 30.046512603759766\n",
      "Eneriges standard deiation: 5.970083236694336\n",
      "Energies mean: -169792.875\n"
     ]
    }
   ],
   "source": [
    "# Normalize training dataset, compute statistics\n",
    "(\n",
    "    (forces_std,),\n",
    "    (energies_mean, energies_std)\n",
    ") = trainer.dataset_train.statistics(\n",
    "    fields=[\n",
    "        AtomicDataDict.FORCE_KEY,\n",
    "        AtomicDataDict.TOTAL_ENERGY_KEY\n",
    "    ],\n",
    "    modes=[\"rms\", \"mean_std\"],\n",
    ")\n",
    "print(f\"Forces standard deviation: {forces_std.item()}\")\n",
    "print(f\"Eneriges standard deiation: {energies_std.item()}\")\n",
    "print(f\"Energies mean: {energies_mean.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy_minus_mean = result[0]['total_energy'] - energies_mean.numpy()\n",
    "# energy_normalized = energy_minus_mean * energies_std.numpy()\n",
    "# trainer.dataset_train.get_data()[0]['total_energy'] = energy_normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-169796.0938])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.dataset_train[0]['total_energy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force and Energy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "from nequip.model._grads import ForceOutput\n",
    "from typing import Optional\n",
    "import logging\n",
    "from e3nn import o3\n",
    "from nequip.data import AtomicDataDict, AtomicDataset\n",
    "from nequip.nn import (\n",
    "    SequentialGraphNetwork,\n",
    "    AtomwiseLinear,\n",
    "    AtomwiseReduce,\n",
    "    ConvNetLayer,\n",
    ")\n",
    "from nequip.nn.embedding import (\n",
    "    OneHotAtomEncoding,\n",
    "    RadialBasisEdgeEncoding,\n",
    "    SphericalHarmonicEdgeAttrs,\n",
    ")\n",
    "\n",
    "from nequip.model import builder_utils\n",
    "\n",
    "def EnergyModel(\n",
    "    config, initialize: bool, dataset: Optional[AtomicDataset] = None\n",
    ") -> SequentialGraphNetwork:\n",
    "    \"\"\"Base default energy model archetecture.\n",
    "\n",
    "    For minimal and full configuration option listings, see ``minimal.yaml`` and ``example.yaml``.\n",
    "    \"\"\"\n",
    "    logging.debug(\"Start building the network model\")\n",
    "\n",
    "    builder_utils.add_avg_num_neighbors(\n",
    "        config=config, initialize=initialize, dataset=dataset\n",
    "    )\n",
    "\n",
    "    num_layers = config.get(\"num_layers\", 3)\n",
    "\n",
    "    layers = {\n",
    "        # -- Encode --\n",
    "        \"one_hot\": OneHotAtomEncoding,\n",
    "        \"spharm_edges\": SphericalHarmonicEdgeAttrs,\n",
    "        \"radial_basis\": RadialBasisEdgeEncoding,\n",
    "        # -- Embed features --\n",
    "        \"chemical_embedding\": AtomwiseLinear,\n",
    "    }\n",
    "\n",
    "    # add convnet layers\n",
    "    # insertion preserves order\n",
    "    for layer_i in range(num_layers):\n",
    "        layers[f\"layer{layer_i}_convnet\"] = ConvNetLayer\n",
    "\n",
    "    # .update also maintains insertion order\n",
    "    layers.update(\n",
    "        {\n",
    "            # TODO: the next linear throws out all L > 0, don't create them in the last layer of convnet\n",
    "            # -- output block --\n",
    "            \"conv_to_output_hidden\": AtomwiseLinear,\n",
    "            \"output_hidden_to_scalar\": (\n",
    "                AtomwiseLinear,\n",
    "                dict(irreps_out=\"1x0e\", out_field=AtomicDataDict.PER_ATOM_ENERGY_KEY),\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    layers[\"total_energy_sum\"] = (\n",
    "        AtomwiseReduce,\n",
    "        dict(\n",
    "            reduce=\"sum\",\n",
    "            field=AtomicDataDict.PER_ATOM_ENERGY_KEY,\n",
    "            out_field=AtomicDataDict.TOTAL_ENERGY_KEY,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return SequentialGraphNetwork.from_parameters(\n",
    "        shared_params=config,\n",
    "        layers=layers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "energy_model = EnergyModel(config, initialize=True, dataset=dataset)\n",
    "force_model = ForceOutput(energy_model)\n",
    "#print(force_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nequip.nn import RescaleOutput\n",
    "final_model = RescaleOutput(\n",
    "    model=force_model,\n",
    "    scale_keys=[AtomicDataDict.TOTAL_ENERGY_KEY, AtomicDataDict.FORCE_KEY],\n",
    "    scale_by=forces_std,\n",
    "    shift_keys=AtomicDataDict.TOTAL_ENERGY_KEY,\n",
    "    shift_by=energies_mean,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: josiahbjorgaard. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9155b0c6fcca4379bba11752e1110550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016678429300009158, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/josiah/Projects/NequIP-Tutorial/wandb/run-20230418_141150-7oq8m0oq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/josiahbjorgaard/nequip-tutorial/runs/7oq8m0oq' target=\"_blank\">fresh-armadillo-4</a></strong> to <a href='https://wandb.ai/josiahbjorgaard/nequip-tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/josiahbjorgaard/nequip-tutorial' target=\"_blank\">https://wandb.ai/josiahbjorgaard/nequip-tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/josiahbjorgaard/nequip-tutorial/runs/7oq8m0oq' target=\"_blank\">https://wandb.ai/josiahbjorgaard/nequip-tutorial/runs/7oq8m0oq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"nequip-tutorial\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of weights: 61696\n",
      "Number of trainable weights: 61696\n",
      "! Starting training ...\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      0     5         1.11         1.11     0.000296         22.8         31.6         15.7           31         23.3         20.4         40.9         30.6         6.06        0.404\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Initial Validation          0    2.270    0.005         1.04     0.000224         1.04         22.2         30.6         15.4           30         22.7         20.2         39.2         29.7         5.06        0.337\n",
      "Wall time: 2.271973335999064\n",
      "! Best model        0    1.036\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      1    10         1.03         1.03      0.00303         22.4         30.5           16         29.6         22.8         20.2         39.1         29.6         24.1         1.61\n",
      "      1    20        0.546        0.515       0.0305         16.1         21.6         11.5         21.5         16.5         15.2         27.1         21.1         78.6         5.24\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      1     5        0.759        0.734       0.0251         18.7         25.7         12.9         25.3         19.1         16.7         33.2           25         71.1         4.74\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               1    7.253    0.005        0.888      0.00657        0.895         20.6         28.3         14.3         27.8         21.1         19.1         36.1         27.6         28.9         1.93\n",
      "! Validation          1    7.253    0.005        0.689        0.024        0.713         18.3         24.9         12.6           25         18.8         16.5         31.9         24.2         69.6         4.64\n",
      "Wall time: 7.256040991996997\n",
      "! Best model        1    0.713\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      2    10        0.485        0.355         0.13         12.8         17.9         8.35         17.9         13.1         11.2         23.3         17.2          162         10.8\n",
      "      2    20        0.207        0.207     0.000401           10         13.7         6.37         14.2         10.3          8.2           18         13.1         8.75        0.583\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      2     5        0.284        0.269       0.0148         11.1         15.6         6.36         16.6         11.5          8.4           21         14.7         54.7         3.65\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               2    9.322    0.005        0.355        0.071        0.426         12.7         17.9         8.08           18           13           11         23.4         17.2          108         7.22\n",
      "! Validation          2    9.322    0.005        0.249       0.0144        0.264         10.7           15         6.11         15.9           11         8.12         20.2         14.1         53.9          3.6\n",
      "Wall time: 9.324784807002288\n",
      "! Best model        2    0.264\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      3    10          0.2        0.192       0.0075         9.67         13.2         6.82         12.9         9.88         8.17         17.2         12.7         38.8         2.59\n",
      "      3    20        0.113        0.113     0.000188         7.69         10.1         5.42         10.3         7.85         6.75         12.9         9.83         5.83        0.389\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      3     5        0.145        0.143      0.00264         8.31         11.4         4.99         12.1         8.54         6.47         15.1         10.8         22.8         1.52\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               3   11.520    0.005        0.171      0.00532        0.176         9.02         12.4         5.77         12.7         9.25         7.35         16.4         11.9         27.4         1.83\n",
      "! Validation          3   11.520    0.005        0.131      0.00269        0.134         8.01         10.9         5.12         11.3         8.22         6.49         14.3         10.4         23.2         1.54\n",
      "Wall time: 11.523031306998746\n",
      "! Best model        3    0.134\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      4    10        0.141         0.14      0.00166         8.27         11.2         5.49         11.4         8.46         7.14         14.5         10.8         18.1         1.21\n",
      "      4    20        0.103        0.103     0.000151         7.09         9.64         4.84         9.66         7.25         6.28         12.4         9.35         4.76        0.317\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      4     5        0.105        0.104     9.17e-05         7.06         9.71         4.18         10.4         7.27          5.5         12.9         9.22         3.65        0.244\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               4   13.912    0.005        0.111      0.00101        0.112         7.38           10         4.77         10.4         7.57          6.2         13.1         9.64         12.6        0.839\n",
      "! Validation          4   13.912    0.005       0.0973     7.81e-05       0.0973         6.86         9.37         4.32         9.76         7.04         5.54         12.4         8.96         3.34        0.223\n",
      "Wall time: 13.915864165996027\n",
      "! Best model        4    0.097\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      5    10        0.115        0.115     0.000636         7.69         10.2         5.65           10         7.83         6.97         12.9         9.93           11        0.733\n",
      "      5    20       0.0772       0.0771     4.71e-05          6.3         8.35          4.5         8.35         6.43         5.59         10.7         8.12         2.66        0.177\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5     5       0.0816       0.0815      0.00012         6.29         8.58         3.84         9.08         6.46         4.94         11.4         8.17         4.44        0.296\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               5   16.464    0.005       0.0863     0.000484       0.0868         6.56         8.83         4.39         9.04         6.72         5.66         11.4         8.54          8.6        0.573\n",
      "! Validation          5   16.464    0.005       0.0764     8.76e-05       0.0765          6.1         8.31         3.93         8.58         6.25         5.04         10.9         7.97         3.57        0.238\n",
      "Wall time: 16.467291888002364\n",
      "! Best model        5    0.076\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      6    10       0.0804       0.0801     0.000333         6.35          8.5         4.55         8.41         6.48         5.82         10.8          8.3         7.75        0.516\n",
      "      6    20       0.0587       0.0586     2.49e-05         5.61         7.28         4.38         7.02          5.7         5.45         8.92         7.18         1.94        0.129\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      6     5       0.0667       0.0666     0.000129         5.66         7.75         3.62         7.99         5.81         4.72         10.2         7.44         4.69        0.312\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               6   18.691    0.005       0.0663     0.000436       0.0668         5.75         7.74         3.96         7.79         5.87         5.17         9.89         7.53         7.67        0.511\n",
      "! Validation          6   18.691    0.005       0.0622     9.03e-05       0.0623         5.53         7.49         3.72          7.6         5.66          4.8         9.69         7.25         3.65        0.243\n",
      "Wall time: 18.69427693200123\n",
      "! Best model        6    0.062\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      7    10       0.0652       0.0646     0.000595         5.76         7.64         4.33          7.4         5.87         5.69         9.37         7.53         10.7        0.714\n",
      "      7    20       0.0462       0.0461     8.41e-05         5.03         6.45         4.04         6.17          5.1         4.94         7.83         6.39          3.8        0.253\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      7     5       0.0525       0.0525     4.51e-05         5.08         6.88         3.42         6.98          5.2         4.49         8.86         6.67         2.62        0.175\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               7   20.929    0.005       0.0528     0.000421       0.0532         5.17         6.91         3.77         6.77         5.27         4.97          8.6         6.78          7.6        0.506\n",
      "! Validation          7   20.929    0.005       0.0498      3.1e-05       0.0498            5          6.7         3.51          6.7          5.1         4.55         8.52         6.54         2.17        0.145\n",
      "Wall time: 20.934915805002674\n",
      "! Best model        7    0.050\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      8    10       0.0531       0.0526     0.000522         5.18         6.89         4.05         6.48         5.27         5.41         8.27         6.84           10        0.668\n",
      "      8    20       0.0335       0.0334     0.000116         4.44         5.49         3.91         5.04         4.48         4.77         6.21         5.49         4.67        0.311\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      8     5       0.0397       0.0397     2.25e-05         4.47         5.99         3.19         5.93         4.56         4.21         7.52         5.86         1.62        0.108\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               8   24.819    0.005       0.0434     0.000276       0.0436         4.71         6.26         3.62         5.96         4.79          4.8         7.58         6.19         6.13        0.408\n",
      "! Validation          8   24.819    0.005        0.039      2.2e-05        0.039         4.45         5.94         3.27         5.79         4.53         4.28         7.38         5.83          1.7        0.113\n",
      "Wall time: 24.82968422999693\n",
      "! Best model        8    0.039\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      9    10       0.0474       0.0471     0.000285         4.94         6.52         3.78         6.26         5.02         5.06         7.86         6.46         7.22        0.481\n",
      "      9    20       0.0292       0.0291     0.000104         4.17         5.13         3.85         4.53         4.19         4.72         5.55         5.14         4.42        0.295\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "      9     5       0.0317       0.0317     1.88e-05         4.04         5.35            3         5.22         4.11         3.97         6.58         5.27         1.39       0.0927\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train               9   29.378    0.005       0.0366     0.000114       0.0367         4.33         5.75         3.46         5.32         4.39          4.6         6.82         5.71         4.05         0.27\n",
      "! Validation          9   29.378    0.005       0.0319     2.21e-05        0.032         4.04         5.37         3.09         5.14         4.11         4.06         6.56         5.31         1.65         0.11\n",
      "Wall time: 29.382172812001954\n",
      "! Best model        9    0.032\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     10    10       0.0406       0.0404     0.000179         4.53         6.04         3.51         5.69          4.6         4.75         7.24         5.99         5.53        0.368\n",
      "     10    20        0.027       0.0269      6.2e-05         3.98         4.93         3.67         4.32            4         4.56         5.31         4.94         3.27        0.218\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     10     5       0.0268       0.0268     1.64e-05         3.73         4.92         2.86         4.73          3.8          3.8         5.95         4.87          1.3       0.0868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              10   32.970    0.005       0.0307     7.08e-05       0.0307         3.97         5.26         3.28         4.76         4.02         4.37         6.12         5.25         3.16        0.211\n",
      "! Validation         10   32.970    0.005       0.0273     1.97e-05       0.0273         3.76         4.96         2.96         4.66         3.81         3.89         5.96         4.92         1.56        0.104\n",
      "Wall time: 32.97340771200106\n",
      "! Best model       10    0.027\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     11    10       0.0344       0.0343     9.24e-05         4.13         5.56         3.31         5.07         4.19          4.5         6.57         5.54         3.59         0.24\n",
      "     11    20       0.0248       0.0247     4.07e-05         3.78         4.73         3.51          4.1          3.8         4.39         5.08         4.74         2.47        0.165\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     11     5       0.0235       0.0235     1.45e-05          3.5          4.6         2.76         4.36         3.56         3.65          5.5         4.57         1.26       0.0841\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              11   36.634    0.005       0.0265      6.8e-05       0.0265         3.69         4.89         3.11         4.35         3.73         4.15         5.62         4.88         3.15         0.21\n",
      "! Validation         11   36.634    0.005        0.024      1.7e-05        0.024         3.53         4.65         2.86          4.3         3.58         3.76          5.5         4.63         1.47       0.0977\n",
      "Wall time: 36.64165315400169\n",
      "! Best model       11    0.024\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     12    10       0.0297       0.0296     7.78e-05         3.82         5.17         3.14         4.59         3.87         4.29         6.02         5.16         3.12        0.208\n",
      "     12    20       0.0232       0.0232     1.53e-05         3.67         4.58         3.41         3.95         3.68         4.28          4.9         4.59         1.31       0.0871\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     12     5       0.0211       0.0211     1.32e-05         3.32         4.36         2.67         4.05         3.36         3.51         5.16         4.34         1.27       0.0844\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              12   39.138    0.005       0.0235     6.13e-05       0.0236         3.49         4.61         2.97         4.08         3.53         3.96         5.26         4.61         2.92        0.195\n",
      "! Validation         12   39.138    0.005       0.0216      1.5e-05       0.0216         3.35         4.41         2.77         4.01         3.39         3.64         5.15          4.4         1.39       0.0926\n",
      "Wall time: 39.143697471998166\n",
      "! Best model       12    0.022\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     13    10       0.0258       0.0258     9.04e-05         3.53         4.82         3.02         4.12         3.57         4.13         5.51         4.82         3.48        0.232\n",
      "     13    20       0.0222       0.0222     1.37e-05         3.61         4.47         3.35          3.9         3.62          4.2         4.77         4.48         1.58        0.106\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     13     5       0.0193       0.0193     1.23e-05         3.17         4.18         2.59         3.84         3.21          3.4         4.92         4.16         1.29       0.0862\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              13   42.696    0.005       0.0215     6.33e-05       0.0216         3.35         4.41         2.86         3.91         3.38          3.8         5.01         4.41         2.93        0.196\n",
      "! Validation         13   42.696    0.005       0.0198     1.35e-05       0.0198         3.21         4.23         2.69          3.8         3.25         3.54          4.9         4.22         1.34       0.0891\n",
      "Wall time: 42.70031011699757\n",
      "! Best model       13    0.020\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     14    10       0.0231        0.023     9.46e-05         3.31         4.56         2.93         3.74         3.33         4.01         5.12         4.56         3.57        0.238\n",
      "     14    20       0.0212       0.0212     2.83e-05         3.53         4.38         3.28         3.82         3.55         4.13         4.64         4.39            2        0.134\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     14     5       0.0181       0.0181     1.19e-05         3.07         4.04         2.52         3.69         3.11          3.3         4.74         4.02         1.33       0.0884\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              14   45.669    0.005       0.0203     7.01e-05       0.0204         3.26         4.29         2.77         3.81         3.29         3.69         4.88         4.28         3.03        0.202\n",
      "! Validation         14   45.669    0.005       0.0185     1.24e-05       0.0185         3.11         4.09         2.63         3.66         3.15         3.44         4.72         4.08         1.29       0.0857\n",
      "Wall time: 45.672085257996514\n",
      "! Best model       14    0.019\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     15    10       0.0215       0.0214     9.01e-05         3.18         4.39         2.86         3.54          3.2         3.92         4.88          4.4         3.42        0.228\n",
      "     15    20       0.0208       0.0208     3.93e-05         3.49         4.33         3.21          3.8         3.51         4.07         4.61         4.34         2.35        0.157\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     15     5       0.0171       0.0171     1.18e-05         2.99         3.93         2.48         3.58         3.03         3.22         4.61         3.92         1.35       0.0899\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "! Train              15   47.744    0.005       0.0195      7.5e-05       0.0196          3.2          4.2         2.71         3.76         3.23          3.6          4.8          4.2         3.09        0.206\n",
      "! Validation         15   47.744    0.005       0.0176     1.15e-05       0.0176         3.03         3.98         2.58         3.55         3.07         3.37         4.59         3.98         1.24       0.0827\n",
      "Wall time: 47.74673959000211\n",
      "! Best model       15    0.018\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     16    10       0.0206       0.0205     8.72e-05         3.13          4.3          2.8         3.52         3.16         3.84         4.77         4.31         3.32        0.221\n",
      "     16    20       0.0208       0.0208     3.85e-05         3.48         4.33         3.14         3.87         3.51            4         4.68         4.34         2.32        0.154\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     16     5       0.0164       0.0164     1.16e-05         2.93         3.84         2.45         3.49         2.97         3.16          4.5         3.83         1.36       0.0906\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              16   49.851    0.005       0.0188     7.36e-05       0.0189         3.14         4.12         2.65          3.7         3.18         3.51         4.72         4.12         3.02        0.201\n",
      "! Validation         16   49.851    0.005       0.0168     1.08e-05       0.0168         2.97          3.9         2.54         3.47            3          3.3         4.48         3.89          1.2       0.0798\n",
      "Wall time: 49.85324254100124\n",
      "! Best model       16    0.017\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     17    10       0.0201         0.02     8.28e-05         3.14         4.25         2.75         3.58         3.16         3.78         4.73         4.26         3.16        0.211\n",
      "     17    20       0.0206       0.0206     3.57e-05         3.47         4.31         3.07         3.92          3.5         3.94          4.7         4.32         2.22        0.148\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     17     5       0.0157       0.0157     1.15e-05         2.88         3.76         2.41         3.41         2.91          3.1          4.4         3.75         1.37       0.0911\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              17   51.945    0.005        0.018     6.86e-05       0.0181         3.07         4.03         2.59         3.63         3.11         3.44         4.62         4.03         2.91        0.194\n",
      "! Validation         17   51.945    0.005       0.0162     1.02e-05       0.0162         2.92         3.82          2.5         3.39         2.95         3.24         4.39         3.82         1.16       0.0773\n",
      "Wall time: 51.94824504899589\n",
      "! Best model       17    0.016\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     18    10       0.0195       0.0194     7.33e-05         3.11         4.18         2.72         3.55         3.14         3.73         4.65         4.19         2.83        0.189\n",
      "     18    20       0.0199       0.0199     3.49e-05          3.4         4.24         3.01         3.84         3.43         3.88         4.62         4.25         2.19        0.146\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     18     5       0.0151       0.0151     1.15e-05         2.83         3.69         2.38         3.34         2.86         3.04         4.31         3.68         1.38       0.0921\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              18   54.084    0.005       0.0171     6.19e-05       0.0172         2.99         3.93         2.53         3.52         3.03         3.36         4.49         3.93         2.78        0.186\n",
      "! Validation         18   54.084    0.005       0.0156     9.76e-06       0.0156         2.87         3.75         2.47         3.32          2.9         3.19         4.31         3.75         1.12       0.0748\n",
      "Wall time: 54.08798404200206\n",
      "! Best model       18    0.016\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     19    10       0.0185       0.0184      6.3e-05         3.03         4.08         2.68         3.44         3.06         3.67          4.5         4.09         2.45        0.163\n",
      "     19    20       0.0191       0.0191     3.53e-05         3.31         4.15         2.95         3.74         3.34         3.82         4.49         4.16         2.21        0.147\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     19     5       0.0145       0.0145     1.17e-05         2.78         3.62         2.35         3.27         2.81            3         4.22         3.61          1.4       0.0931\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              19   56.186    0.005       0.0163     5.55e-05       0.0163         2.92         3.83         2.47         3.43         2.95          3.3         4.36         3.83         2.65        0.176\n",
      "! Validation         19   56.186    0.005       0.0151     9.42e-06       0.0151         2.82         3.69         2.43         3.26         2.85         3.14         4.23         3.69         1.09       0.0729\n",
      "Wall time: 56.188515629000904\n",
      "! Best model       19    0.015\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     20    10       0.0176       0.0176     5.39e-05         2.96         3.98         2.64         3.32         2.98         3.62         4.36         3.99         2.21        0.148\n",
      "     20    20       0.0184       0.0184     3.58e-05         3.24         4.07         2.89         3.64         3.27         3.77         4.39         4.08         2.22        0.148\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     20     5        0.014        0.014     1.17e-05         2.74         3.56         2.32         3.21         2.77         2.95         4.14         3.55          1.4       0.0934\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              20   58.368    0.005       0.0156     5.08e-05       0.0157         2.86         3.76         2.43         3.35         2.89         3.24         4.27         3.76         2.53        0.169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "! Validation         20   58.368    0.005       0.0146     9.13e-06       0.0146         2.78         3.63          2.4         3.21          2.8          3.1         4.16         3.63         1.07       0.0715\n",
      "Wall time: 58.37118184599967\n",
      "! Best model       20    0.015\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     21    10       0.0169       0.0169     4.59e-05         2.91          3.9         2.62         3.24         2.93         3.58         4.24         3.91         2.04        0.136\n",
      "     21    20       0.0178       0.0178     3.58e-05         3.18         4.01         2.83         3.57          3.2         3.72         4.32         4.02         2.23        0.149\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     21     5       0.0136       0.0135     1.17e-05         2.69          3.5         2.29         3.15         2.72         2.91         4.06         3.49          1.4       0.0932\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              21   61.025    0.005       0.0152     4.77e-05       0.0152         2.82          3.7         2.39          3.3         2.85         3.19          4.2          3.7         2.46        0.164\n",
      "! Validation         21   61.025    0.005       0.0142     8.91e-06       0.0142         2.74         3.58         2.37         3.15         2.76         3.06         4.09         3.58         1.06       0.0705\n",
      "Wall time: 61.02928552099911\n",
      "! Best model       21    0.014\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     22    10       0.0164       0.0163     3.97e-05         2.86         3.84         2.59         3.17         2.88         3.54         4.16         3.85         1.94        0.129\n",
      "     22    20       0.0174       0.0173     3.51e-05         3.12         3.96         2.78         3.51         3.15         3.67         4.26         3.96         2.21        0.147\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     22     5       0.0131       0.0131     1.16e-05         2.65         3.44         2.27          3.1         2.68         2.87            4         3.43          1.4        0.093\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              22   63.959    0.005       0.0148     4.53e-05       0.0148         2.78         3.65         2.36         3.26         2.81         3.15         4.15         3.65          2.4         0.16\n",
      "! Validation         22   63.959    0.005       0.0138     8.72e-06       0.0138          2.7         3.53         2.34          3.1         2.72         3.02         4.03         3.53         1.04       0.0696\n",
      "Wall time: 63.96547544100031\n",
      "! Best model       22    0.014\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     23    10       0.0159       0.0159     3.56e-05         2.83         3.79         2.57         3.13         2.85          3.5         4.09          3.8         1.85        0.124\n",
      "     23    20        0.017       0.0169     3.37e-05         3.07         3.91         2.73         3.47          3.1         3.62         4.22         3.92         2.16        0.144\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     23     5       0.0128       0.0127     1.15e-05         2.62         3.39         2.24         3.05         2.64         2.83         3.93         3.38         1.39       0.0925\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              23   67.019    0.005       0.0144     4.32e-05       0.0145         2.75         3.61         2.33         3.23         2.78         3.11          4.1         3.61         2.35        0.157\n",
      "! Validation         23   67.019    0.005       0.0134     8.54e-06       0.0134         2.66         3.48         2.31         3.06         2.69         2.99         3.97         3.48         1.03        0.069\n",
      "Wall time: 67.02215607599646\n",
      "! Best model       23    0.013\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     24    10       0.0156       0.0155     3.32e-05         2.81         3.75         2.55          3.1         2.83         3.47         4.04         3.75         1.77        0.118\n",
      "     24    20       0.0166       0.0165     3.18e-05         3.03         3.86         2.68         3.43         3.06         3.57         4.17         3.87         2.08        0.139\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     24     5       0.0124       0.0124     1.13e-05         2.58         3.35         2.21         3.01         2.61          2.8         3.88         3.34         1.38       0.0918\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              24   70.200    0.005       0.0141     4.11e-05       0.0141         2.71         3.56          2.3         3.19         2.74         3.07         4.06         3.56         2.31        0.154\n",
      "! Validation         24   70.200    0.005       0.0131     8.36e-06       0.0131         2.63         3.44         2.29         3.02         2.65         2.96         3.92         3.44         1.02       0.0683\n",
      "Wall time: 70.20432556799642\n",
      "! Best model       24    0.013\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     25    10       0.0153       0.0152     3.21e-05         2.79         3.71         2.53         3.08         2.81         3.44            4         3.72         1.84        0.123\n",
      "     25    20       0.0162       0.0162     2.97e-05         2.99         3.82         2.64          3.4         3.02         3.52         4.13         3.83         2.02        0.134\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     25     5       0.0121       0.0121     1.11e-05         2.55          3.3         2.18         2.97         2.57         2.77         3.82          3.3         1.37       0.0911\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              25   72.554    0.005       0.0137     3.91e-05       0.0138         2.68         3.52         2.27         3.15         2.71         3.03         4.01         3.52         2.25         0.15\n",
      "! Validation         25   72.554    0.005       0.0128      8.2e-06       0.0128          2.6          3.4         2.26         2.98         2.62         2.93         3.87          3.4         1.02       0.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wall time: 72.55659110000124\n",
      "! Best model       25    0.013\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     26    10        0.015       0.0149     3.18e-05         2.77         3.67         2.51         3.06         2.79         3.41         3.96         3.68         1.99        0.133\n",
      "     26    20       0.0158       0.0158     2.77e-05         2.95         3.78          2.6         3.36         2.98         3.48         4.09         3.78         1.95         0.13\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     26     5       0.0118       0.0118      1.1e-05         2.52         3.26         2.15         2.93         2.54         2.74         3.77         3.26         1.35       0.0902\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              26   74.876    0.005       0.0134     3.72e-05       0.0135         2.65         3.48         2.24         3.11         2.68            3         3.96         3.48          2.2        0.147\n",
      "! Validation         26   74.876    0.005       0.0125     8.05e-06       0.0126         2.57         3.37         2.24         2.95         2.59          2.9         3.83         3.36         1.01       0.0672\n",
      "Wall time: 74.87889300000097\n",
      "! Best model       26    0.013\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     27    10       0.0147       0.0147      3.2e-05         2.75         3.64         2.49         3.05         2.77         3.38         3.92         3.65         2.12        0.142\n",
      "     27    20       0.0154       0.0154     2.59e-05         2.91         3.73         2.56         3.31         2.93         3.43         4.05         3.74         1.89        0.126\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     27     5       0.0115       0.0115     1.08e-05         2.49         3.22         2.12          2.9         2.51         2.71         3.72         3.22         1.34       0.0894\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              27   76.882    0.005       0.0131     3.55e-05       0.0132         2.62         3.44         2.21         3.07         2.64         2.96         3.92         3.44         2.14        0.143\n",
      "! Validation         27   76.882    0.005       0.0123     7.91e-06       0.0123         2.54         3.33         2.21         2.92         2.56         2.88         3.79         3.33            1       0.0668\n",
      "Wall time: 76.88569002199802\n",
      "! Best model       27    0.012\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     28    10       0.0144       0.0144     3.27e-05         2.73          3.6         2.47         3.02         2.75         3.35         3.87         3.61         2.24        0.149\n",
      "     28    20       0.0151        0.015     2.42e-05         2.86         3.69         2.52         3.26         2.89         3.39            4         3.69         1.83        0.122\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     28     5       0.0113       0.0112     1.06e-05         2.46         3.19          2.1         2.87         2.48         2.69         3.67         3.18         1.33       0.0886\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              28   78.916    0.005       0.0128     3.39e-05       0.0129         2.59          3.4         2.19         3.04         2.61         2.93         3.87          3.4         2.09        0.139\n",
      "! Validation         28   78.916    0.005       0.0121     7.78e-06       0.0121         2.52          3.3         2.19         2.88         2.54         2.85         3.75          3.3        0.996       0.0664\n",
      "Wall time: 78.91835776299558\n",
      "! Best model       28    0.012\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     29    10       0.0141       0.0141     3.35e-05         2.71         3.56         2.45         2.99         2.72         3.32         3.83         3.57         2.34        0.156\n",
      "     29    20       0.0147       0.0147     2.26e-05         2.82         3.64         2.48         3.21         2.85         3.35         3.94         3.65         1.77        0.118\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     29     5        0.011        0.011     1.04e-05         2.43         3.15         2.07         2.83         2.45         2.66         3.63         3.15         1.32       0.0877\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              29   80.980    0.005       0.0126     3.26e-05       0.0126         2.56         3.37         2.17            3         2.58         2.91         3.83         3.37         2.03        0.136\n",
      "! Validation         29   80.980    0.005       0.0118     7.64e-06       0.0118         2.49         3.27         2.17         2.86         2.51         2.83         3.71         3.27        0.989       0.0659\n",
      "Wall time: 80.98315160100174\n",
      "! Best model       29    0.012\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     30    10       0.0138       0.0138     3.45e-05         2.68         3.53         2.43         2.96          2.7         3.29         3.78         3.53         2.43        0.162\n",
      "     30    20       0.0143       0.0143     2.11e-05         2.78         3.59         2.44         3.17         2.81         3.31         3.89          3.6         1.71        0.114\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     30     5       0.0108       0.0108     1.02e-05          2.4         3.12         2.05          2.8         2.43         2.64         3.58         3.11          1.3       0.0866\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              30   83.100    0.005       0.0123     3.16e-05       0.0123         2.53         3.33         2.15         2.97         2.56         2.88         3.78         3.33         1.99        0.133\n",
      "! Validation         30   83.100    0.005       0.0116     7.51e-06       0.0116         2.47         3.24         2.15         2.83         2.49         2.81         3.67         3.24        0.981       0.0654\n",
      "Wall time: 83.10609930600185\n",
      "! Best model       30    0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     31    10       0.0135       0.0135     3.56e-05         2.66         3.49         2.41         2.93         2.67         3.26         3.73          3.5         2.51        0.167\n",
      "     31    20       0.0139       0.0139     1.95e-05         2.75         3.54         2.41         3.14         2.77         3.27         3.83         3.55         1.64         0.11\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     31     5       0.0106       0.0105     1.01e-05         2.38         3.09         2.03         2.77          2.4         2.62         3.54         3.08         1.29       0.0859\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              31   85.225    0.005        0.012     3.06e-05       0.0121          2.5          3.3         2.13         2.93         2.53         2.85         3.74          3.3         1.95         0.13\n",
      "! Validation         31   85.225    0.005       0.0114      7.4e-06       0.0114         2.44         3.21         2.13          2.8         2.47         2.79         3.64         3.21        0.976        0.065\n",
      "Wall time: 85.22813014600251\n",
      "! Best model       31    0.011\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     32    10       0.0132       0.0132     3.68e-05         2.63         3.45         2.39          2.9         2.65         3.24         3.68         3.46         2.58        0.172\n",
      "     32    20       0.0135       0.0135     1.79e-05         2.71         3.49         2.37         3.09         2.73         3.23         3.77          3.5         1.58        0.105\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     32     5       0.0103       0.0103     9.86e-06         2.35         3.05         2.02         2.73         2.37          2.6          3.5         3.05         1.27       0.0849\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              32   87.323    0.005       0.0118     2.97e-05       0.0118         2.48         3.26         2.11          2.9          2.5         2.83          3.7         3.26         1.91        0.128\n",
      "! Validation         32   87.323    0.005       0.0112      7.3e-06       0.0113         2.42         3.19         2.12         2.77         2.44         2.77         3.61         3.19        0.968       0.0645\n",
      "Wall time: 87.3260551209969\n",
      "! Best model       32    0.011\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     33    10       0.0129       0.0129      3.8e-05          2.6         3.41         2.37         2.86         2.62         3.21         3.62         3.42         2.65        0.176\n",
      "     33    20       0.0131       0.0131     1.63e-05         2.67         3.44         2.33         3.05         2.69         3.19         3.71         3.45          1.5          0.1\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     33     5       0.0101       0.0101     9.63e-06         2.33         3.02            2          2.7         2.35         2.58         3.46         3.02         1.26       0.0839\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              33   89.412    0.005       0.0115     2.89e-05       0.0116         2.45         3.23         2.09         2.87         2.48          2.8         3.65         3.23         1.88        0.125\n",
      "! Validation         33   89.412    0.005       0.0111     7.16e-06       0.0111          2.4         3.16          2.1         2.75         2.42         2.75         3.57         3.16        0.959       0.0639\n",
      "Wall time: 89.41512158200203\n",
      "! Best model       33    0.011\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     34    10       0.0126       0.0126     3.92e-05         2.57         3.37         2.35         2.82         2.59         3.18         3.57         3.38          2.7         0.18\n",
      "     34    20       0.0128       0.0128     1.47e-05         2.63         3.39          2.3            3         2.65         3.15         3.65          3.4         1.42        0.095\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     34     5      0.00995      0.00994     9.44e-06          2.3            3         1.98         2.67         2.32         2.56         3.43         2.99         1.24       0.0827\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              34   91.550    0.005       0.0113     2.81e-05       0.0113         2.43         3.19         2.07         2.84         2.45         2.77         3.61         3.19         1.84        0.123\n",
      "! Validation         34   91.550    0.005       0.0109     7.05e-06       0.0109         2.38         3.13         2.08         2.72          2.4         2.73         3.54         3.14         0.95       0.0633\n",
      "Wall time: 91.55417713600036\n",
      "! Best model       34    0.011\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     35    10       0.0123       0.0123     4.02e-05         2.54         3.33         2.33         2.78         2.56         3.16         3.51         3.33         2.75        0.183\n",
      "     35    20       0.0124       0.0124     1.32e-05         2.59         3.34         2.26         2.96         2.61         3.11         3.59         3.35         1.35       0.0898\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     35     5      0.00976      0.00975     9.19e-06         2.28         2.97         1.96         2.64          2.3         2.54         3.39         2.96         1.22       0.0816\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              35   93.797    0.005        0.011     2.72e-05       0.0111          2.4         3.16         2.05         2.81         2.43         2.75         3.57         3.16          1.8         0.12\n",
      "! Validation         35   93.797    0.005       0.0107     6.94e-06       0.0107         2.36         3.11         2.07          2.7         2.38         2.71         3.51         3.11        0.942       0.0628\n",
      "Wall time: 93.80297525400238\n",
      "! Best model       35    0.011\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     36    10        0.012       0.0119     4.11e-05         2.52         3.28         2.32         2.74         2.53         3.13         3.45         3.29         2.78        0.185\n",
      "     36    20        0.012        0.012     1.19e-05         2.54         3.29         2.23         2.91         2.57         3.07         3.53          3.3         1.28       0.0854\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     36     5      0.00958      0.00957     8.94e-06         2.25         2.94         1.94         2.61         2.28         2.52         3.36         2.94         1.21       0.0804\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              36   97.760    0.005       0.0108     2.62e-05       0.0108         2.38         3.12         2.03         2.78          2.4         2.73         3.52         3.12         1.77        0.118\n",
      "! Validation         36   97.760    0.005       0.0106     6.83e-06       0.0106         2.34         3.09         2.05         2.67         2.36         2.69         3.48         3.09        0.932       0.0621\n",
      "Wall time: 97.76554210000177\n",
      "! Best model       36    0.011\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     37    10       0.0116       0.0116     4.16e-05         2.48         3.24         2.29          2.7          2.5          3.1         3.38         3.24         2.79        0.186\n",
      "     37    20       0.0116       0.0116     1.07e-05          2.5         3.24         2.19         2.85         2.52         3.03         3.46         3.25         1.23       0.0823\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     37     5       0.0094       0.0094     8.72e-06         2.23         2.91         1.92         2.58         2.25          2.5         3.32         2.91         1.19       0.0793\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              37  100.574    0.005       0.0106     2.52e-05       0.0106         2.35         3.09         2.01         2.74         2.38          2.7         3.48         3.09         1.73        0.115\n",
      "! Validation         37  100.574    0.005       0.0104     6.71e-06       0.0104         2.32         3.06         2.04         2.65         2.34         2.67         3.46         3.06        0.924       0.0616\n",
      "Wall time: 100.57825992899598\n",
      "! Best model       37    0.010\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     38    10       0.0113       0.0113     4.15e-05         2.45         3.19         2.27         2.65         2.46         3.07         3.31         3.19         2.79        0.186\n",
      "     38    20       0.0113       0.0112     9.64e-06         2.46         3.19         2.16          2.8         2.48         2.99         3.39         3.19         1.19       0.0792\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     38     5      0.00924      0.00923     8.49e-06         2.21         2.89         1.91         2.56         2.23         2.48         3.29         2.88         1.17       0.0781\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              38  103.836    0.005       0.0103      2.4e-05       0.0103         2.32         3.05         1.99          2.7         2.35         2.67         3.43         3.05         1.68        0.112\n",
      "! Validation         38  103.836    0.005       0.0102      6.6e-06       0.0102          2.3         3.04         2.02         2.63         2.32         2.65         3.43         3.04        0.916        0.061\n",
      "Wall time: 103.83909102100006\n",
      "! Best model       38    0.010\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     39    10       0.0109       0.0109     4.07e-05          2.4         3.13         2.24         2.59         2.42         3.04         3.23         3.14         2.76        0.184\n",
      "     39    20       0.0109       0.0109     8.71e-06         2.41         3.13         2.13         2.73         2.43         2.96         3.32         3.14         1.14       0.0763\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     39     5      0.00907      0.00906     8.23e-06         2.19         2.86         1.89         2.53         2.21         2.46         3.26         2.86         1.15       0.0769\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              39  106.679    0.005         0.01     2.26e-05       0.0101          2.3         3.01         1.97         2.66         2.32         2.65         3.38         3.01         1.64        0.109\n",
      "! Validation         39  106.679    0.005       0.0101     6.49e-06       0.0101         2.29         3.02         2.01          2.6         2.31         2.63          3.4         3.02        0.907       0.0605\n",
      "Wall time: 106.68284306099667\n",
      "! Best model       39    0.010\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     40    10       0.0105       0.0105     3.91e-05         2.36         3.07         2.21         2.54         2.37            3         3.15         3.08         2.71        0.181\n",
      "     40    20       0.0105       0.0105     7.82e-06         2.36         3.08          2.1         2.65         2.38         2.92         3.25         3.08         1.09       0.0729\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     40     5      0.00891       0.0089     8.03e-06         2.17         2.83         1.87          2.5         2.19         2.44         3.23         2.83         1.14       0.0757\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              40  109.878    0.005      0.00976     2.12e-05      0.00978         2.26         2.97         1.95         2.62         2.29         2.62         3.32         2.97         1.59        0.106\n",
      "! Validation         40  109.878    0.005      0.00992      6.4e-06      0.00993         2.27         2.99         1.99         2.58         2.29         2.62         3.37         2.99        0.899       0.0599\n",
      "Wall time: 109.88084235399583\n",
      "! Best model       40    0.010\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     41    10       0.0101       0.0101     3.68e-05         2.31         3.01         2.17         2.47         2.32         2.97         3.07         3.02         2.62        0.175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     41    20       0.0101       0.0101     6.93e-06          2.3         3.02         2.07         2.57         2.32         2.88         3.18         3.03         1.04       0.0692\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     41     5      0.00875      0.00874     7.78e-06         2.14         2.81         1.85         2.47         2.16         2.42          3.2         2.81         1.12       0.0745\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              41  111.905    0.005      0.00946     1.97e-05      0.00948         2.23         2.92         1.93         2.57         2.25         2.59         3.26         2.93         1.54        0.103\n",
      "! Validation         41  111.905    0.005      0.00977      6.3e-06      0.00977         2.25         2.97         1.98         2.56         2.27          2.6         3.34         2.97        0.891       0.0594\n",
      "Wall time: 111.90730206599983\n",
      "! Best model       41    0.010\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     42    10      0.00971      0.00968      3.4e-05         2.26         2.96         2.13         2.41         2.27         2.93         2.99         2.96          2.5        0.167\n",
      "     42    20      0.00981      0.00981     6.06e-06         2.26         2.98         2.05         2.49         2.27         2.84         3.13         2.98        0.984       0.0656\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     42     5      0.00859      0.00859     7.52e-06         2.12         2.78         1.84         2.45         2.14          2.4         3.17         2.78          1.1       0.0731\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              42  114.076    0.005      0.00914      1.8e-05      0.00916         2.19         2.87          1.9         2.52         2.21         2.55          3.2         2.88         1.48       0.0987\n",
      "! Validation         42  114.076    0.005      0.00961     6.21e-06      0.00962         2.23         2.95         1.96         2.54         2.25         2.58         3.32         2.95        0.883       0.0589\n",
      "Wall time: 114.0788833129991\n",
      "! Best model       42    0.010\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     43    10      0.00937      0.00934     3.09e-05         2.21          2.9          2.1         2.35         2.22         2.89         2.92          2.9         2.36        0.157\n",
      "     43    20      0.00955      0.00954     5.32e-06         2.22         2.94         2.03         2.42         2.23          2.8         3.09         2.94        0.928       0.0619\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     43     5      0.00844      0.00843     7.25e-06          2.1         2.76         1.82         2.43         2.12         2.38         3.14         2.76         1.08       0.0717\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              43  116.084    0.005      0.00883     1.63e-05      0.00884         2.15         2.82         1.87         2.47         2.17         2.52         3.14         2.83         1.42       0.0947\n",
      "! Validation         43  116.084    0.005      0.00946      6.1e-06      0.00946         2.21         2.92         1.94         2.52         2.23         2.56         3.29         2.92        0.873       0.0582\n",
      "Wall time: 116.08735271600017\n",
      "! Best model       43    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     44    10      0.00908      0.00905     2.81e-05         2.17         2.86         2.07          2.3         2.18         2.85         2.87         2.86          2.2        0.147\n",
      "     44    20      0.00931      0.00931     4.81e-06         2.18          2.9         2.01         2.38         2.19         2.76         3.05         2.91        0.884        0.059\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     44     5      0.00828      0.00828     6.98e-06         2.08         2.73          1.8          2.4          2.1         2.36         3.11         2.73         1.05       0.0703\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              44  118.263    0.005      0.00853     1.47e-05      0.00855         2.11         2.78         1.85         2.42         2.13         2.48         3.08         2.78         1.36       0.0905\n",
      "! Validation         44  118.263    0.005       0.0093        6e-06       0.0093          2.2          2.9         1.93          2.5         2.21         2.54         3.26          2.9        0.863       0.0575\n",
      "Wall time: 118.26750631599862\n",
      "! Best model       44    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     45    10      0.00884      0.00881     2.57e-05         2.14         2.82         2.04         2.26         2.15         2.81         2.83         2.82         2.05        0.136\n",
      "     45    20       0.0091      0.00909      4.5e-06         2.14         2.87         1.98         2.34         2.16         2.71         3.03         2.87         0.85       0.0567\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     45     5      0.00813      0.00812      6.7e-06         2.06         2.71         1.79         2.38         2.08         2.34         3.08         2.71         1.03       0.0688\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              45  120.648    0.005      0.00827     1.33e-05      0.00829         2.08         2.73         1.82         2.38          2.1         2.45         3.03         2.74          1.3       0.0863\n",
      "! Validation         45  120.648    0.005      0.00914      5.9e-06      0.00914         2.18         2.87         1.91         2.48          2.2         2.52         3.23         2.87        0.852       0.0568\n",
      "Wall time: 120.6511799880027\n",
      "! Best model       45    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     46    10      0.00862      0.00859     2.39e-05         2.12         2.79         2.02         2.24         2.13         2.77          2.8         2.79         1.91        0.127\n",
      "     46    20      0.00889      0.00889     4.25e-06         2.11         2.83         1.94          2.3         2.12         2.67         3.01         2.84        0.822       0.0548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     46     5      0.00798      0.00797     6.42e-06         2.04         2.68         1.77         2.36         2.06         2.32         3.05         2.68         1.01       0.0672\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              46  122.729    0.005      0.00805     1.22e-05      0.00806         2.05          2.7         1.79         2.34         2.07         2.42         2.98          2.7         1.24       0.0827\n",
      "! Validation         46  122.729    0.005      0.00897      5.8e-06      0.00898         2.16         2.85         1.89         2.46         2.18          2.5          3.2         2.85        0.843       0.0562\n",
      "Wall time: 122.73176027099544\n",
      "! Best model       46    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     47    10      0.00842       0.0084     2.27e-05          2.1         2.75            2         2.22         2.11         2.74         2.77         2.76         1.86        0.124\n",
      "     47    20      0.00869      0.00869     3.96e-06         2.07          2.8          1.9         2.27         2.09         2.63         2.99         2.81        0.788       0.0525\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     47     5      0.00782      0.00781     6.14e-06         2.02         2.66         1.75         2.34         2.04         2.29         3.02         2.66        0.984       0.0656\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              47  124.888    0.005      0.00785     1.14e-05      0.00786         2.03         2.66         1.77         2.31         2.04         2.38         2.95         2.67         1.21       0.0805\n",
      "! Validation         47  124.888    0.005      0.00881     5.72e-06      0.00881         2.14         2.82         1.87         2.44         2.16         2.48         3.17         2.82        0.833       0.0555\n",
      "Wall time: 124.89107840100041\n",
      "! Best model       47    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     48    10      0.00826      0.00823     2.19e-05         2.09         2.73         1.98         2.21         2.09          2.7         2.76         2.73         1.82        0.121\n",
      "     48    20      0.00851       0.0085     3.56e-06         2.04         2.77         1.87         2.25         2.06         2.58         2.97         2.78        0.747       0.0498\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     48     5      0.00767      0.00766     5.83e-06            2         2.63         1.72         2.32         2.02         2.27         2.99         2.63        0.958       0.0639\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              48  127.226    0.005      0.00768      1.1e-05      0.00769            2         2.63         1.75         2.29         2.02         2.35         2.92         2.64         1.19       0.0791\n",
      "! Validation         48  127.226    0.005      0.00864     5.61e-06      0.00864         2.12         2.79         1.85         2.42         2.14         2.45         3.13         2.79        0.821       0.0547\n",
      "Wall time: 127.22975266999856\n",
      "! Best model       48    0.009\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     49    10      0.00811      0.00809     2.14e-05         2.07          2.7         1.96          2.2         2.08         2.66         2.75         2.71         1.79        0.119\n",
      "     49    20      0.00833      0.00833     3.08e-06         2.02         2.74         1.83         2.24         2.03         2.54         2.95         2.75        0.709       0.0473\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     49     5      0.00751      0.00751     5.54e-06         1.98          2.6          1.7          2.3            2         2.25         2.96          2.6         0.93        0.062\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              49  129.350    0.005      0.00753      1.1e-05      0.00754         1.99         2.61         1.73         2.27            2         2.33          2.9         2.61         1.19       0.0794\n",
      "! Validation         49  129.350    0.005      0.00847     5.52e-06      0.00848          2.1         2.77         1.83         2.41         2.12         2.43          3.1         2.77         0.81        0.054\n",
      "Wall time: 129.3528351060013\n",
      "! Best model       49    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     50    10        0.008      0.00798     2.09e-05         2.06         2.68         1.93          2.2         2.06         2.62         2.75         2.69         1.76        0.117\n",
      "     50    20      0.00817      0.00817     2.57e-06            2         2.72          1.8         2.23         2.01          2.5         2.94         2.72        0.659        0.044\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     50     5      0.00736      0.00736     5.23e-06         1.96         2.58         1.68         2.28         1.98         2.22         2.93         2.58          0.9         0.06\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              50  131.678    0.005       0.0074     1.14e-05      0.00741         1.97         2.59         1.72         2.26         1.99          2.3         2.88         2.59         1.21       0.0808\n",
      "! Validation         50  131.678    0.005       0.0083     5.42e-06      0.00831         2.08         2.74         1.81         2.39          2.1         2.41         3.07         2.74        0.797       0.0531\n",
      "Wall time: 131.68128602900106\n",
      "! Best model       50    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     51    10      0.00793       0.0079     2.04e-05         2.04         2.67          1.9          2.2         2.05         2.59         2.76         2.68         1.75        0.117\n",
      "     51    20      0.00803      0.00803     2.15e-06         1.98         2.69         1.77         2.23            2         2.47         2.93          2.7        0.609       0.0406\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     51     5      0.00722      0.00721     4.94e-06         1.94         2.55         1.66         2.26         1.96          2.2          2.9         2.55        0.872       0.0581\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              51  135.074    0.005      0.00729     1.21e-05       0.0073         1.96         2.57          1.7         2.25         1.97         2.27         2.86         2.57         1.24       0.0829\n",
      "! Validation         51  135.074    0.005      0.00814     5.35e-06      0.00814         2.06         2.71         1.79         2.37         2.08         2.38         3.04         2.71        0.787       0.0525\n",
      "Wall time: 135.08038762400247\n",
      "! Best model       51    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     52    10      0.00787      0.00785     1.98e-05         2.03         2.66         1.88         2.21         2.04         2.55         2.78         2.67         1.74        0.116\n",
      "     52    20       0.0079       0.0079     1.94e-06         1.97         2.67         1.75         2.23         1.99         2.44         2.92         2.68        0.553       0.0369\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     52     5      0.00707      0.00707     4.64e-06         1.92         2.53         1.63         2.24         1.94         2.17         2.88         2.53        0.842       0.0561\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              52  138.537    0.005      0.00719     1.29e-05       0.0072         1.94         2.55         1.68         2.24         1.96         2.25         2.85         2.55         1.28        0.085\n",
      "! Validation         52  138.537    0.005      0.00798     5.26e-06      0.00798         2.04         2.68         1.77         2.35         2.06         2.36         3.01         2.69        0.777       0.0518\n",
      "Wall time: 138.5412258150027\n",
      "! Best model       52    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     53    10      0.00781      0.00779     1.93e-05         2.02         2.65         1.85         2.22         2.03         2.52          2.8         2.66         1.74        0.116\n",
      "     53    20       0.0078       0.0078     2.05e-06         1.96         2.65         1.74         2.23         1.98         2.41         2.91         2.66        0.528       0.0352\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     53     5      0.00693      0.00693     4.38e-06          1.9          2.5         1.61         2.22         1.92         2.15         2.85          2.5        0.814       0.0543\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              53  142.064    0.005      0.00709     1.37e-05       0.0071         1.93         2.53         1.67         2.23         1.95         2.22         2.84         2.53         1.32        0.088\n",
      "! Validation         53  142.064    0.005      0.00782     5.18e-06      0.00783         2.02         2.66         1.75         2.33         2.04         2.33         2.99         2.66        0.768       0.0512\n",
      "Wall time: 142.06692503499653\n",
      "! Best model       53    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     54    10      0.00773      0.00771     1.88e-05         2.01         2.64         1.82         2.22         2.02         2.48         2.81         2.64         1.75        0.117\n",
      "     54    20      0.00772      0.00772     2.62e-06         1.96         2.64         1.73         2.22         1.98         2.39          2.9         2.64        0.544       0.0362\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     54     5       0.0068       0.0068     4.13e-06         1.88         2.48         1.59         2.21          1.9         2.12         2.83         2.48        0.786       0.0524\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              54  145.813    0.005      0.00698     1.45e-05        0.007         1.92         2.51         1.65         2.22         1.94          2.2         2.82         2.51         1.36       0.0905\n",
      "! Validation         54  145.813    0.005      0.00767      5.1e-06      0.00768            2         2.63         1.73         2.31         2.02         2.31         2.96         2.63         0.76       0.0507\n",
      "Wall time: 145.81909657499637\n",
      "! Best model       54    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     55    10      0.00761      0.00759     1.85e-05         1.99         2.62         1.79         2.21            2         2.45          2.8         2.62         1.76        0.117\n",
      "     55    20      0.00766      0.00766     3.82e-06         1.96         2.63         1.74         2.22         1.98         2.38         2.89         2.63        0.703       0.0469\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     55     5      0.00667      0.00667     3.93e-06         1.86         2.45         1.57         2.19         1.88          2.1          2.8         2.45        0.761       0.0507\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              55  148.652    0.005      0.00686      1.5e-05      0.00688          1.9         2.49         1.64         2.21         1.92         2.18          2.8         2.49         1.39       0.0925\n",
      "! Validation         55  148.652    0.005      0.00753     5.02e-06      0.00753         1.98         2.61         1.71          2.3            2         2.28         2.93         2.61        0.752       0.0501\n",
      "Wall time: 148.65509131499857\n",
      "! Best model       55    0.008\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     56    10      0.00746      0.00744     1.84e-05         1.97         2.59         1.76          2.2         1.98         2.42         2.78          2.6         1.77        0.118\n",
      "     56    20      0.00763      0.00763     5.67e-06         1.97         2.62         1.75         2.22         1.99         2.37         2.88         2.63        0.947       0.0631\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     56     5      0.00655      0.00654     3.69e-06         1.84         2.43         1.55         2.18         1.86         2.08         2.78         2.43        0.733       0.0489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              56  151.901    0.005      0.00674     1.53e-05      0.00675         1.89         2.47         1.62         2.19          1.9         2.15         2.78         2.47         1.41       0.0941\n",
      "! Validation         56  151.901    0.005      0.00738     4.92e-06      0.00739         1.96         2.58         1.69         2.28         1.98         2.26         2.91         2.58        0.742       0.0495\n",
      "Wall time: 151.90556310399552\n",
      "! Best model       56    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     57    10      0.00732      0.00731     1.84e-05         1.95         2.57         1.74         2.19         1.97         2.39         2.76         2.57         1.77        0.118\n",
      "     57    20       0.0076       0.0076     7.73e-06         1.98         2.62         1.77         2.23            2         2.37         2.88         2.62         1.16       0.0773\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     57     5      0.00643      0.00642     3.53e-06         1.82         2.41         1.53         2.16         1.85         2.05         2.76         2.41        0.711       0.0474\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              57  154.698    0.005       0.0066     1.55e-05      0.00662         1.87         2.44          1.6         2.18         1.89         2.12         2.76         2.44         1.43       0.0951\n",
      "! Validation         57  154.698    0.005      0.00725     4.85e-06      0.00725         1.95         2.56         1.67         2.27         1.97         2.23         2.89         2.56        0.735        0.049\n",
      "Wall time: 154.7015182979958\n",
      "! Best model       57    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     58    10      0.00722       0.0072     1.84e-05         1.94         2.55         1.73         2.19         1.96         2.36         2.75         2.56         1.77        0.118\n",
      "     58    20      0.00755      0.00754      9.4e-06         1.99         2.61         1.78         2.23            2         2.36         2.87         2.61         1.31       0.0873\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     58     5      0.00631      0.00631     3.36e-06         1.81         2.39         1.51         2.15         1.83         2.03         2.74         2.38        0.694       0.0462\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              58  156.831    0.005      0.00648     1.56e-05      0.00649         1.85         2.42         1.58         2.16         1.87          2.1         2.74         2.42         1.44       0.0957\n",
      "! Validation         58  156.831    0.005      0.00712     4.76e-06      0.00712         1.93         2.53         1.65         2.25         1.95         2.21         2.86         2.54        0.727       0.0485\n",
      "Wall time: 156.83424279899918\n",
      "! Best model       58    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     59    10      0.00714      0.00712     1.86e-05         1.94         2.54         1.71         2.19         1.95         2.34         2.74         2.54         1.77        0.118\n",
      "     59    20      0.00748      0.00747     1.05e-05         1.99          2.6         1.79         2.22         2.01         2.36         2.85          2.6         1.39       0.0929\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     59     5       0.0062       0.0062     3.21e-06         1.79         2.37         1.49         2.14         1.81            2         2.72         2.36        0.675        0.045\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              59  159.250    0.005      0.00636     1.59e-05      0.00637         1.83          2.4         1.56         2.15         1.85         2.07         2.72          2.4         1.44       0.0961\n",
      "! Validation         59  159.250    0.005      0.00699     4.66e-06      0.00699         1.91         2.51         1.63         2.24         1.93         2.18         2.84         2.51        0.721       0.0481\n",
      "Wall time: 159.25269066899637\n",
      "! Best model       59    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     60    10      0.00707      0.00705     1.88e-05         1.93         2.52         1.69         2.19         1.94         2.31         2.74         2.53         1.77        0.118\n",
      "     60    20       0.0074      0.00739      1.1e-05         1.99         2.58          1.8         2.21            2         2.35         2.83         2.59         1.44       0.0962\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     60     5      0.00609      0.00609     3.06e-06         1.78         2.34         1.47         2.12          1.8         1.98          2.7         2.34        0.658       0.0439\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              60  161.338    0.005      0.00625     1.61e-05      0.00627         1.82         2.38         1.54         2.14         1.84         2.05          2.7         2.37         1.45       0.0967\n",
      "! Validation         60  161.338    0.005      0.00686     4.57e-06      0.00687          1.9         2.49         1.61         2.22         1.92         2.16         2.82         2.49        0.715       0.0476\n",
      "Wall time: 161.34170127599646\n",
      "! Best model       60    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     61    10      0.00701      0.00699     1.91e-05         1.92         2.51         1.67          2.2         1.94         2.29         2.75         2.52         1.77        0.118\n",
      "     61    20      0.00732       0.0073     1.12e-05         1.99         2.57          1.8          2.2            2         2.34         2.81         2.57         1.46       0.0971\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     61     5      0.00598      0.00598     2.95e-06         1.76         2.32         1.46         2.11         1.78         1.96         2.68         2.32        0.647       0.0431\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "! Train              61  163.686    0.005      0.00615     1.64e-05      0.00616          1.8         2.36         1.52         2.12         1.82         2.03         2.68         2.35         1.45       0.0969\n",
      "! Validation         61  163.686    0.005      0.00674     4.48e-06      0.00675         1.88         2.47         1.59         2.21          1.9         2.13          2.8         2.47        0.709       0.0473\n",
      "Wall time: 163.6892322720014\n",
      "! Best model       61    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     62    10      0.00696      0.00694     1.95e-05         1.91          2.5         1.66          2.2         1.93         2.26         2.75         2.51         1.77        0.118\n",
      "     62    20      0.00722      0.00721     1.09e-05         1.98         2.55          1.8         2.19            2         2.32         2.79         2.56         1.44       0.0962\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     62     5      0.00588      0.00588     2.81e-06         1.75          2.3         1.44          2.1         1.77         1.93         2.67          2.3        0.633       0.0422\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              62  166.043    0.005      0.00605     1.67e-05      0.00607         1.79         2.34         1.51         2.11         1.81            2         2.67         2.34         1.47       0.0978\n",
      "! Validation         62  166.043    0.005      0.00663     4.39e-06      0.00663         1.86         2.45         1.57         2.19         1.88         2.11         2.78         2.45        0.703       0.0469\n",
      "Wall time: 166.0463696509978\n",
      "! Best model       62    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     63    10       0.0069      0.00688        2e-05         1.91         2.49         1.64         2.21         1.93         2.24         2.76          2.5          1.8         0.12\n",
      "     63    20      0.00713      0.00712     1.05e-05         1.98         2.54          1.8         2.18         1.99         2.31         2.77         2.54         1.42       0.0944\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     63     5      0.00578      0.00578     2.68e-06         1.73         2.28         1.42         2.09         1.75         1.91         2.65         2.28        0.617       0.0411\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              63  168.130    0.005      0.00596      1.7e-05      0.00598         1.78         2.32         1.49          2.1          1.8         1.98         2.66         2.32         1.48       0.0988\n",
      "! Validation         63  168.130    0.005      0.00651     4.28e-06      0.00652         1.85         2.42         1.55         2.18         1.87         2.08         2.76         2.42        0.695       0.0464\n",
      "Wall time: 168.1334166500019\n",
      "! Best model       63    0.007\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     64    10      0.00686      0.00684     2.05e-05          1.9         2.48         1.62         2.22         1.92         2.21         2.76         2.49         1.83        0.122\n",
      "     64    20      0.00703      0.00702     9.94e-06         1.97         2.52          1.8         2.16         1.98          2.3         2.75         2.52         1.37       0.0915\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     64     5      0.00568      0.00568     2.59e-06         1.72         2.26          1.4         2.07         1.74         1.88         2.63         2.26        0.606       0.0404\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              64  170.350    0.005      0.00587     1.74e-05      0.00589         1.76          2.3         1.48         2.09         1.78         1.96         2.64          2.3          1.5          0.1\n",
      "! Validation         64  170.350    0.005       0.0064     4.19e-06      0.00641         1.83          2.4         1.54         2.16         1.85         2.06         2.74          2.4        0.688       0.0459\n",
      "Wall time: 170.3526528479997\n",
      "! Best model       64    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     65    10      0.00681      0.00679     2.12e-05          1.9         2.48          1.6         2.23         1.92         2.19         2.77         2.48         1.87        0.124\n",
      "     65    20      0.00693      0.00693     9.37e-06         1.96          2.5         1.79         2.15         1.97         2.28         2.73         2.51         1.34       0.0892\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     65     5      0.00559      0.00558      2.5e-06          1.7         2.25         1.39         2.06         1.72         1.86         2.62         2.24        0.595       0.0397\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              65  172.502    0.005      0.00579     1.78e-05      0.00581         1.75         2.29         1.46         2.08         1.77         1.94         2.63         2.28         1.52        0.101\n",
      "! Validation         65  172.502    0.005      0.00629     4.09e-06       0.0063         1.81         2.38         1.52         2.15         1.83         2.04         2.73         2.38        0.682       0.0455\n",
      "Wall time: 172.50489412200113\n",
      "! Best model       65    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     66    10      0.00676      0.00673     2.19e-05         1.89         2.47         1.58         2.24         1.91         2.16         2.77         2.47          1.9        0.126\n",
      "     66    20      0.00684      0.00683     8.82e-06         1.95         2.48         1.79         2.14         1.96         2.27         2.71         2.49         1.29       0.0862\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     66     5      0.00549      0.00549     2.41e-06         1.69         2.23         1.37         2.05         1.71         1.84          2.6         2.22        0.584        0.039\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              66  174.697    0.005       0.0057     1.82e-05      0.00572         1.74         2.27         1.45         2.07         1.76         1.92         2.61         2.27         1.54        0.102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "! Validation         66  174.697    0.005      0.00619        4e-06      0.00619          1.8         2.36          1.5         2.13         1.82         2.01         2.71         2.36        0.674        0.045\n",
      "Wall time: 174.70303757299553\n",
      "! Best model       66    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     67    10      0.00671      0.00668     2.27e-05         1.88         2.46         1.57         2.25         1.91         2.14         2.77         2.46         1.93        0.129\n",
      "     67    20      0.00674      0.00673     8.33e-06         1.94         2.46         1.78         2.13         1.95         2.25         2.69         2.47         1.25       0.0835\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     67     5      0.00541       0.0054      2.3e-06         1.67         2.21         1.36         2.03         1.69         1.81         2.59          2.2         0.57        0.038\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              67  177.603    0.005      0.00562     1.87e-05      0.00564         1.73         2.25         1.43         2.06         1.75          1.9          2.6         2.25         1.55        0.104\n",
      "! Validation         67  177.603    0.005      0.00609     3.91e-06      0.00609         1.78         2.34         1.49         2.12          1.8         1.99         2.69         2.34        0.666       0.0444\n",
      "Wall time: 177.60867813500226\n",
      "! Best model       67    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     68    10      0.00665      0.00663     2.37e-05         1.88         2.45         1.55         2.25          1.9         2.12         2.77         2.45         1.97        0.131\n",
      "     68    20      0.00664      0.00663     7.88e-06         1.93         2.45         1.77         2.11         1.94         2.23         2.67         2.45         1.22       0.0815\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     68     5      0.00532      0.00532     2.24e-06         1.66         2.19         1.34         2.02         1.68         1.79         2.57         2.18        0.562       0.0375\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              68  181.997    0.005      0.00554     1.91e-05      0.00556         1.72         2.24         1.42         2.05         1.74         1.88         2.58         2.23         1.57        0.104\n",
      "! Validation         68  181.997    0.005      0.00599     3.82e-06      0.00599         1.76         2.33         1.47          2.1         1.79         1.97         2.68         2.32         0.66        0.044\n",
      "Wall time: 182.00348338799813\n",
      "! Best model       68    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     69    10       0.0066      0.00657     2.48e-05         1.87         2.44         1.53         2.26          1.9          2.1         2.77         2.44         1.99        0.133\n",
      "     69    20      0.00654      0.00654     7.47e-06         1.92         2.43         1.77          2.1         1.93         2.22         2.65         2.43         1.19       0.0792\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     69     5      0.00524      0.00523     2.17e-06         1.64         2.17         1.32         2.01         1.67         1.77         2.56         2.16        0.553       0.0369\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              69  186.673    0.005      0.00545     1.95e-05      0.00547          1.7         2.22         1.41         2.04         1.72         1.86         2.57         2.21         1.58        0.105\n",
      "! Validation         69  186.673    0.005      0.00589     3.73e-06       0.0059         1.75         2.31         1.45         2.09         1.77         1.95         2.66          2.3        0.653       0.0436\n",
      "Wall time: 186.6789464159956\n",
      "! Best model       69    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     70    10      0.00655      0.00652      2.6e-05         1.86         2.43         1.52         2.26         1.89         2.07         2.77         2.42         2.03        0.135\n",
      "     70    20      0.00645      0.00645      7.1e-06         1.91         2.41         1.76         2.09         1.92          2.2         2.63         2.42         1.16       0.0771\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     70     5      0.00516      0.00515     2.12e-06         1.63         2.16         1.31         1.99         1.65         1.75         2.55         2.15        0.547       0.0365\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              70  190.452    0.005      0.00537        2e-05      0.00539         1.69          2.2         1.39         2.03         1.71         1.84         2.55          2.2         1.59        0.106\n",
      "! Validation         70  190.452    0.005       0.0058     3.65e-06       0.0058         1.73         2.29         1.44         2.07         1.75         1.93         2.64         2.28        0.649       0.0432\n",
      "Wall time: 190.4545623419981\n",
      "! Best model       70    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     71    10      0.00649      0.00647     2.73e-05         1.86         2.42          1.5         2.27         1.88         2.05         2.77         2.41         2.07        0.138\n",
      "     71    20      0.00637      0.00636     6.77e-06          1.9          2.4         1.75         2.08         1.91         2.19         2.61          2.4         1.12        0.075\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "     71     5      0.00508      0.00508     2.06e-06         1.62         2.14         1.29         1.98         1.64         1.73         2.53         2.13        0.536       0.0357\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n",
      "! Train              71  192.533    0.005      0.00529     2.04e-05      0.00531         1.68         2.19         1.38         2.02          1.7         1.83         2.54         2.18          1.6        0.106\n",
      "! Validation         71  192.533    0.005      0.00571     3.55e-06      0.00571         1.72         2.27         1.42         2.06         1.74         1.91         2.63         2.27         0.64       0.0427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wall time: 192.53644884999812\n",
      "! Best model       71    0.006\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse      H_f_mae      C_f_mae  psavg_f_mae     H_f_rmse     C_f_rmse psavg_f_rmse        e_mae      e/N_mae\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-27e073de2b6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nequip/train/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_cond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_of_epoch_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nequip/train/trainer.py\u001b[0m in \u001b[0;36mepoch_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    914\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m                     self.batch_step(\n\u001b[0m\u001b[1;32m    917\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m                         \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mVALIDATION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nequip/train/trainer.py\u001b[0m in \u001b[0;36mbatch_step\u001b[0;34m(self, data, validation)\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_from_model_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         }\n\u001b[0;32m--> 817\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nequip/nn/_rescale.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAtomicDataDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAtomicDataDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/nequip/nn/_grad_output.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Get grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         grads = torch.autograd.grad(\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;31m# TODO:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# This makes sense for scalar batch-level or batch-wise outputs, specifically because d(sum(batches))/d wrt = sum(d batch / d wrt) = d my_batch / d wrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trainer.model = final_model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./benchmark_data/toluene_ccsd_t-train.npz\n"
     ]
    }
   ],
   "source": [
    "# Inference on last frame in dataset\n",
    "print(config.dataset_file_name)\n",
    "toluene_data = np.load(config.dataset_file_name)\n",
    "r = toluene_data['R'][0]\n",
    "forces = toluene_data['F'][0]\n",
    "\n",
    "ATOMIC_NUMBERS_KEY = torch.Tensor(torch.from_numpy(toluene_data['z'].astype(np.float32))).to(torch.int64)\n",
    "\n",
    "#The atomic types for toluene as trained in the model are 1: Carbon, 0: Hydrogen, \n",
    "# and so those must match here. Previously, all atoms were mapped to type 0, causing a mismatch between\n",
    "# the captioned forces.\n",
    "atom_types = (ATOMIC_NUMBERS_KEY == 6).to(torch.int64)\n",
    "data = AtomicData.from_points(\n",
    "    pos=r,\n",
    "    r_max=config['r_max'], \n",
    "    **{AtomicDataDict.ATOMIC_NUMBERS_KEY: ATOMIC_NUMBERS_KEY,\n",
    "    AtomicDataDict.ATOM_TYPE_KEY: atom_types\n",
    "    }\n",
    ")\n",
    "#print(data.atomic_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_model.train(); \n",
    "pred = force_model(AtomicData.to_AtomicDataDict(data))['forces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot original and predicted force vectors\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.plot(\n",
    "    r[:, 0],\n",
    "    r[:, 1],\n",
    "    '.k',\n",
    "    markersize=10,\n",
    ")\n",
    "\n",
    "plt.quiver(\n",
    "    r[:, 0],\n",
    "    r[:, 1],\n",
    "    pred[:, 0].detach().numpy(),\n",
    "    pred[:, 1].detach().numpy(),\n",
    "    norm=None\n",
    ")\n",
    "\n",
    "plt.quiver(\n",
    "    r[:, 0], \n",
    "    r[:, 1], \n",
    "    forces[:, 0],\n",
    "    forces[:, 1],\n",
    "    color='red', \n",
    "    norm=None\n",
    ")\n",
    "\n",
    "plt.legend(['Positions', 'Predicted', 'True'], prop={'size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MD Simulation with LAMMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile lammps\n",
    "# !wget \"https://github.com/lammps/lammps/archive/stable.zip\"\n",
    "# !unzip -q stable.zip\n",
    "# !rm stable.zip\n",
    "# !mv lammps-stable lammps\n",
    "# !wget \"https://github.com/mir-group/pair_nequip/archive/main.zip\"\n",
    "# !unzip -q main.zip\n",
    "# !rm main.zip\n",
    "# !mv pair_nequip-main pair_nequip\n",
    "# !cd pair_nequip && ./patch_lammps.sh ../lammps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'lammps'...\n",
      "remote: Enumerating objects: 11732, done.\u001b[K\n",
      "remote: Counting objects: 100% (11732/11732), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8603/8603), done.\u001b[K\n",
      "remote: Total 11732 (delta 3943), reused 6318 (delta 2930), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (11732/11732), 110.00 MiB | 12.46 MiB/s, done.\n",
      "Resolving deltas: 100% (3943/3943), done.\n",
      "Note: switching to '7586adbb6a61254125992709ef2fda9134cfca6c'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "Updating files: 100% (11058/11058), done.\n",
      "--2023-03-22 10:50:44--  https://github.com/mir-group/pair_nequip/archive/main.zip\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/mir-group/pair_nequip/zip/refs/heads/main [following]\n",
      "--2023-03-22 10:50:45--  https://codeload.github.com/mir-group/pair_nequip/zip/refs/heads/main\n",
      "Resolving codeload.github.com (codeload.github.com)... 192.30.255.121\n",
      "Connecting to codeload.github.com (codeload.github.com)|192.30.255.121|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘main.zip’\n",
      "\n",
      "main.zip                [ <=>                ] 188.57K  1.15MB/s    in 0.2s    \n",
      "\n",
      "2023-03-22 10:50:45 (1.15 MB/s) - ‘main.zip’ saved [193094]\n",
      "\n",
      "Copying files...\n",
      "Updating CMakeLists.txt...\n",
      "sed: 1: \"../lammps/cmake/CMakeLi ...\": invalid command code .\n",
      "Done!\n",
      "-- The CXX compiler identification is Clang 12.0.0\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /opt/homebrew/Caskroom/miniforge/base/bin/arm64-apple-darwin20.0.0-clang++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.37.1 (Apple Git-137.1)\") \n",
      "-- Running check for auto-generated files from make-based build system\n",
      "-- Found MPI_CXX: /opt/homebrew/Cellar/open-mpi/4.1.5/lib/libmpi.dylib (found version \"3.1\") \n",
      "-- Found MPI: TRUE (found version \"3.1\")  \n",
      "-- Looking for C++ include omp.h\n",
      "-- Looking for C++ include omp.h - found\n",
      "-- Found OpenMP_CXX: -fopenmp=libomp (found version \"5.0\") \n",
      "-- Found OpenMP: TRUE (found version \"5.0\")  \n",
      "-- Found JPEG: /opt/homebrew/Caskroom/miniforge/base/lib/libjpeg.dylib (found version \"90\") \n",
      "-- Found PNG: /opt/homebrew/Caskroom/miniforge/base/lib/libpng.dylib (found version \"1.6.37\") \n",
      "-- Found ZLIB: /opt/homebrew/Caskroom/miniforge/base/lib/libz.dylib (found version \"1.2.13\") \n",
      "-- Found GZIP: /usr/bin/gzip  \n",
      "-- Could NOT find FFMPEG (missing: FFMPEG_EXECUTABLE) \n",
      "-- Looking for C++ include cmath\n",
      "-- Looking for C++ include cmath - found\n",
      "-- Generating style headers...\n",
      "-- Generating package headers...\n",
      "-- Generating lmpinstalledpkgs.h...\n",
      "-- Could NOT find ClangFormat (missing: ClangFormat_EXECUTABLE) (Required is at least version \"8.0\")\n",
      "-- The following tools and libraries have been found and configured:\n",
      " * Git\n",
      " * MPI\n",
      " * OpenMP\n",
      " * JPEG\n",
      " * PNG\n",
      " * ZLIB\n",
      "\n",
      "-- <<< Build configuration >>>\n",
      "   Operating System: Darwin  \n",
      "   Build type:       \n",
      "   Install path:     /Users/ingrid/.local\n",
      "   Generator:        Unix Makefiles using /opt/homebrew/Caskroom/miniforge/base/bin/make\n",
      "-- Enabled packages: <None>\n",
      "-- <<< Compilers and Flags: >>>\n",
      "-- C++ Compiler:     /opt/homebrew/Caskroom/miniforge/base/bin/arm64-apple-darwin20.0.0-clang++\n",
      "      Type:          Clang\n",
      "      Version:       12.0.0\n",
      "      C++ Flags:    -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -std=c++14 -fmessage-length=0 -isystem /opt/homebrew/Caskroom/miniforge/base/include \n",
      "      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=4;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP\n",
      "-- <<< Linker flags: >>>\n",
      "-- Executable name:  lmp\n",
      "-- Executable linker flags: -Wl,-pie -Wl,-headerpad_max_install_names -Wl,-dead_strip_dylibs -Wl,-rpath,/opt/homebrew/Caskroom/miniforge/base/lib -L/opt/homebrew/Caskroom/miniforge/base/lib\n",
      "-- Static library flags:    \n",
      "-- <<< MPI flags >>>\n",
      "-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n",
      "-- MPI includes:     /opt/homebrew/Cellar/open-mpi/4.1.5/include\n",
      "-- MPI libraries:    /opt/homebrew/Cellar/open-mpi/4.1.5/lib/libmpi.dylib;\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Caffe2: Found protobuf with new-style protobuf targets.\n",
      "-- Caffe2: Protobuf version 3.19.4\n",
      "\u001b[33mCMake Warning at /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n",
      "  static library kineto_LIBRARY-NOTFOUND not found.\n",
      "Call Stack (most recent call first):\n",
      "  /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/share/cmake/Torch/TorchConfig.cmake:127 (append_torchlib_if_found)\n",
      "  CMakeLists.txt:922 (find_package)\n",
      "\n",
      "\u001b[0m\n",
      "-- Found Torch: /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/lib/libtorch.dylib  \n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /Users/ingrid/PycharmProjects/NequIP/lammps/build\n",
      "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n",
      "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n",
      "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n",
      "-- Git Directory: /Users/ingrid/PycharmProjects/NequIP/lammps/.git\n",
      "[  1%] Built target atom.h\n",
      "[  1%] Built target angle.h\n",
      "[  1%] Built target variable.h\n",
      "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n",
      "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n",
      "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n",
      "[  1%] Built target citeme.h\n",
      "[  1%] Built target bond.h\n",
      "[  1%] Built target comm.h\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n",
      "[  2%] Built target dihedral.h\n",
      "[  2%] Built target compute.h\n",
      "[  2%] Built target domain.h\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n",
      "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n",
      "[  2%] Built target error.h\n",
      "[  2%] Built target fix.h\n",
      "[  2%] Built target force.h\n",
      "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n",
      "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n",
      "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n",
      "[  3%] Built target group.h\n",
      "[  3%] Built target input.h\n",
      "[  3%] Built target improper.h\n",
      "-- Generating lmpgitversion.h...\n",
      "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n",
      "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n",
      "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n",
      "[  4%] Built target gitversion\n",
      "[  4%] Built target info.h\n",
      "[  4%] Built target kspace.h\n",
      "[  4%] Built target lammps.h\n",
      "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n",
      "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n",
      "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n",
      "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n",
      "[  5%] Built target lattice.h\n",
      "[  5%] Built target lmppython.h\n",
      "[  5%] Built target library.h\n",
      "[  5%] Built target lmptype.h\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n",
      "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n",
      "[  6%] Built target memory.h\n",
      "[  6%] Built target modify.h\n",
      "[  6%] Built target neighbor.h\n",
      "[  6%] Built target neigh_list.h\n",
      "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n",
      "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n",
      "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n",
      "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n",
      "[  7%] Built target output.h\n",
      "[  7%] Built target pair.h\n",
      "[  7%] Built target pointers.h\n",
      "[  7%] Built target region.h\n",
      "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n",
      "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n",
      "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n",
      "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n",
      "[  8%] Built target timer.h\n",
      "[  8%] Built target utils.h\n",
      "[  8%] Built target universe.h\n",
      "[  8%] Built target update.h\n",
      "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/angle.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/angle_hybrid.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/angle_deprecated.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/angle_zero.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/arg_info.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/atom.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/atom_map.cpp.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/atom_vec.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/atom_vec_body.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/atom_vec_line.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/balance.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/body.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/bond.cpp.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/bond_deprecated.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/bond_hybrid.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/bond_zero.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/change_box.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/citeme.cpp.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/comm.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/comm_brick.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/comm_tiled.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute.cpp.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_angle.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_angle_local.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_bond.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_bond_local.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_com.cpp.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_deprecated.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_dihedral.cpp.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_dipole.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_global_atom.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_group_group.cpp.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_gyration.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_improper.cpp.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_improper_local.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_ke.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_msd.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_pair.cpp.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_pair_local.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_pe.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_pressure.cpp.o\u001b[0m\n",
      "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_property_atom.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_property_local.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_rdf.cpp.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_reduce.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_slice.cpp.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_temp.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_temp_com.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_temp_region.cpp.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_vacf.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/create_atoms.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/create_bonds.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/create_box.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/delete_atoms.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/delete_bonds.cpp.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/deprecated.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dihedral.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dihedral_zero.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/displace_atoms.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/domain.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump.cpp.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_atom.cpp.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp.o\u001b[0m\n",
      "\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump.cpp:560:7: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "      sprintf(filecurrent,\"%s\" BIGINT_FORMAT \"%s\",\n",
      "\u001b[0;1;32m      ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump.cpp:565:7: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "      sprintf(pad,\"%%s%%0%d%s%%s\",padflag,&bif[1]);\n",
      "\u001b[0;1;32m      ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump.cpp:566:7: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "      sprintf(filecurrent,pad,filestar,update->ntimestep,ptr+1);\n",
      "\u001b[0;1;32m      ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_atom.cpp:518:15: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "    offset += sprintf(&sbuf[offset],format,\n",
      "\u001b[0;1;32m              ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_atom.cpp:544:15: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "    offset += sprintf(&sbuf[offset],format,\n",
      "\u001b[0;1;32m              ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m2 warnings generated.\n",
      "\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:165:21: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "          offset += sprintf(&sbuf[offset],\"%f \\n\",mybuf[m]);\n",
      "\u001b[0;1;32m                    ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:167:21: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "          offset += sprintf(&sbuf[offset],\"%s \\n\",typenames[(int) mybuf[m]]);\n",
      "\u001b[0;1;32m                    ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:171:15: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "              sprintf(&sbuf[offset],vformat[j],static_cast<int> (mybuf[m]));\n",
      "\u001b[0;1;32m              ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:173:23: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "            offset += sprintf(&sbuf[offset],vformat[j],mybuf[m]);\n",
      "\u001b[0;1;32m                      ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:176:15: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "              sprintf(&sbuf[offset],vformat[j],typenames[(int) mybuf[m]]);\n",
      "\u001b[0;1;32m              ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:179:15: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "              sprintf(&sbuf[offset],vformat[j],static_cast<bigint> (mybuf[m]));\n",
      "\u001b[0;1;32m              ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:183:17: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "      offset += sprintf(&sbuf[offset],\"\\n\");\n",
      "\u001b[0;1;32m                ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:197:21: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "          offset += sprintf(&sbuf[offset],\"%f \\n\",mybuf[m]);\n",
      "\u001b[0;1;32m                    ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:199:21: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "          offset += sprintf(&sbuf[offset],\"%s \\n\",typenames[(int) mybuf[m]]);\n",
      "\u001b[0;1;32m                    ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:202:21: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "          offset += sprintf(&sbuf[offset],vformat[j],unwrap_coord);\n",
      "\u001b[0;1;32m                    ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:206:15: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "              sprintf(&sbuf[offset],vformat[j],static_cast<int> (mybuf[m]));\n",
      "\u001b[0;1;32m              ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:208:23: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "            offset += sprintf(&sbuf[offset],vformat[j],mybuf[m]);\n",
      "\u001b[0;1;32m                      ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:211:15: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "              sprintf(&sbuf[offset],vformat[j],typenames[(int) mybuf[m]]);\n",
      "\u001b[0;1;32m              ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0me[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_custom.cpp.o\u001b[0m\n",
      "xpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:214:15: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "              sprintf(&sbuf[offset],vformat[j],static_cast<bigint> (mybuf[m]));\n",
      "\u001b[0;1;32m              ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_cfg.cpp:218:17: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "      offset += sprintf(&sbuf[offset],\"\\n\");\n",
      "\u001b[0;1;32m                ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m15 warnings generated.\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_deprecated.cpp.o\u001b[0m\n",
      "3 warnings generated.\n",
      "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_image.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_local.cpp.o\u001b[0m\n",
      "\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_custom.cpp:1195:19: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "        offset += sprintf(&sbuf[offset],vformat[j],static_cast<int> (mybuf[m]));\n",
      "\u001b[0;1;32m                  ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_custom.cpp:1197:19: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "        offset += sprintf(&sbuf[offset],vformat[j],mybuf[m]);\n",
      "\u001b[0;1;32m                  ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_custom.cpp:1199:19: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "        offset += sprintf(&sbuf[offset],vformat[j],typenames[(int) mybuf[m]]);\n",
      "\u001b[0;1;32m                  ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_custom.cpp:1201:19: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "        offset += sprintf(&sbuf[offset],vformat[j],\n",
      "\u001b[0;1;32m                  ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_custom.cpp:1205:15: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "    offset += sprintf(&sbuf[offset],\"\\n\");\n",
      "\u001b[0;1;32m              ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_custom.cpp:1716:7: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "      sprintf(str,\"%s\",BIGINT_FORMAT);\n",
      "\u001b[0;1;32m      ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_custom.cpp:1718:7: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "      sprintf(format_bigint_user,\"%s%s%s\",format_int_user,&str[1],ptr+1);\n",
      "\u001b[0;1;32m      ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_movie.cpp.o\u001b[0m\n",
      "\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_local.cpp:250:7: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "      sprintf(str,\"%s\",BIGINT_FORMAT);\n",
      "\u001b[0;1;32m      ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_local.cpp:252:7: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "      sprintf(format_bigint_user,\"%s%s%s\",format_int_user,&str[1],ptr+1);\n",
      "\u001b[0;1;32m      ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_local.cpp:376:19: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "        offset += sprintf(&sbuf[offset],vformat[j],static_cast<int> (mybuf[m]));\n",
      "\u001b[0;1;32m                  ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_local.cpp:378:19: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "        offset += sprintf(&sbuf[offset],vformat[j],mybuf[m]);\n",
      "\u001b[0;1;32m                  ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_local.cpp:380:19: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "        offset += sprintf(&sbuf[offset],vformat[j],static_cast<bigint> (mybuf[m]));\n",
      "\u001b[0;1;32m                  ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_local.cpp:382:19: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "        offset += sprintf(&sbuf[offset],vformat[j],mybuf[m]);\n",
      "\u001b[0;1;32m                  ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_local.cpp:385:15: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "    offset += sprintf(&sbuf[offset],\"\\n\");\n",
      "\u001b[0;1;32m              ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_xyz.cpp.o\u001b[0m\n",
      "7 warnings generated.\n",
      "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/error.cpp.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/finish.cpp.o\u001b[0m\n",
      "\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_xyz.cpp:87:7: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "      sprintf(typenames[itype],\"%d\",itype);\n",
      "\u001b[0;1;32m      ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/dump_xyz.cpp:179:15: \u001b[0m\u001b[0;1;35mwarning: \u001b[0m\u001b[1m'sprintf' is deprecated: This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead. [-Wdeprecated-declarations]\u001b[0m\n",
      "    offset += sprintf(&sbuf[offset],format,\n",
      "\u001b[0;1;32m              ^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/stdio.h:188:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'sprintf' has been explicitly marked deprecated here\u001b[0m\n",
      "__deprecated_msg(\"This function is provided for compatibility reasons only.  Due to security concerns inherent in the design of sprintf(3), it is highly recommended that you use snprintf(3) instead.\")\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[1m/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/cdefs.h:215:48: \u001b[0m\u001b[0;1;30mnote: \u001b[0mexpanded from macro '__deprecated_msg'\u001b[0m\n",
      "        #define __deprecated_msg(_msg) __attribute__((__deprecated__(_msg)))\n",
      "\u001b[0;1;32m                                                      ^\n",
      "\u001b[0m2 warnings generated.\n",
      "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_adapt.cpp.o\u001b[0m\n",
      "7 warnings generated.\n",
      "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_addforce.cpp.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_ave_time.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_aveforce.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_balance.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_box_relax.cpp.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_deform.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_deposit.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_deprecated.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_dummy.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_efield.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_evaporate.cpp.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_external.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_gravity.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_group.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_halt.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_heat.cpp.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_indent.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_langevin.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_minimize.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_lineforce.cpp.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_momentum.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_move.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_nh.cpp.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_nph.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_npt.cpp.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_nve.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_nvt.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_planeforce.cpp.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_print.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_property_atom.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_read_restart.cpp.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_recenter.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_respa.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_restrain.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_setforce.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_spring.cpp.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_spring_self.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_store.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_store_force.cpp.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_store_state.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_vector.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_viscous.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_wall.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n",
      "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fix_wall_region.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fmtlib_format.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/fmtlib_os.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/gridcomm.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/group.cpp.o\u001b[0m\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp:22:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/build/styles/style_pair.h:16:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/pair_nequip.h:25:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3:\n",
      "\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4:2: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mC++14 or later compatible compiler is required to use PyTorch.\u001b[0m\n",
      "#error C++14 or later compatible compiler is required to use PyTorch.\n",
      "\u001b[0;1;32m ^\n",
      "\u001b[0mIn file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp:22:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/build/styles/style_pair.h:16:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/pair_nequip.h:25:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
      "\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/ATen/ATen.h:4:2: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mC++14 or later compatible compiler is required to use ATen.\u001b[0m\n",
      "#error C++14 or later compatible compiler is required to use ATen.\n",
      "\u001b[0;1;32m ^\n",
      "\u001b[0mIn file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp:22:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/build/styles/style_pair.h:16:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/pair_nequip.h:25:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/ATen/ATen.h:7:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Allocator.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Device.h:5:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/Exception.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/StringUtil.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/string_view.h:4:\n",
      "\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:27:2: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mYou need C++14 to compile PyTorch\u001b[0m\n",
      "#error You need C++14 to compile PyTorch\n",
      "\u001b[0;1;32m ^\n",
      "\u001b[0m\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:70:7: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mno template named 'conditional_t' in namespace 'std'; did you mean 'fmt::conditional_t'?\u001b[0m\n",
      "    : std::conditional_t<bool(B1::value), conjunction<Bn...>, B1> {};\n",
      "\u001b[0;1;32m      ^~~~~\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/fmt/core.h:267:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'fmt::conditional_t' declared here\u001b[0m\n",
      "using conditional_t = typename std::conditional<B, T, F>::type;\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0mIn file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp:22:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/build/styles/style_pair.h:16:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/pair_nequip.h:25:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/ATen/ATen.h:7:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Allocator.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Device.h:5:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/Exception.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/StringUtil.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/string_view.h:4:\n",
      "\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:79:7: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mno template named 'conditional_t' in namespace 'std'; did you mean 'fmt::conditional_t'?\u001b[0m\n",
      "    : std::conditional_t<bool(B1::value), B1, disjunction<Bn...>> {};\n",
      "\u001b[0;1;32m      ^~~~~\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/fmt/core.h:267:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'fmt::conditional_t' declared here\u001b[0m\n",
      "using conditional_t = typename std::conditional<B, T, F>::type;\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0mIn file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp:22:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/build/styles/style_pair.h:16:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/pair_nequip.h:25:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/ATen/ATen.h:7:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Allocator.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Device.h:5:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/Exception.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/StringUtil.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/string_view.h:4:\n",
      "\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:144:10: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mno template named 'index_sequence' in namespace 'std'\u001b[0m\n",
      "    std::index_sequence<INDEX...>)\n",
      "\u001b[0;1;32m    ~~~~~^\n",
      "\u001b[0m\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:141:28: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mdeduced return types are a C++14 extension\u001b[0m\n",
      "CUDA_HOST_DEVICE constexpr decltype(auto) apply_impl(\n",
      "\u001b[0;1;32m                           ^\n",
      "\u001b[0m\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:152:28: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mdeduced return types are a C++14 extension\u001b[0m\n",
      "CUDA_HOST_DEVICE constexpr decltype(auto) apply(F&& f, Tuple&& t) {\n",
      "\u001b[0;1;32m                           ^\n",
      "\u001b[0m\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:156:12: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mno member named 'make_index_sequence' in namespace 'std'\u001b[0m\n",
      "      std::make_index_sequence<\n",
      "\u001b[0;1;32m      ~~~~~^\n",
      "\u001b[0m\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:157:27: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mno template named 'remove_reference_t' in namespace 'std'; did you mean 'fmt::remove_reference_t'?\u001b[0m\n",
      "          std::tuple_size<std::remove_reference_t<Tuple>>::value>{});\n",
      "\u001b[0;1;32m                          ^~~~~\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/fmt/core.h:270:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'fmt::remove_reference_t' declared here\u001b[0m\n",
      "using remove_reference_t = typename std::remove_reference<T>::type;\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0mIn file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp:22:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/build/styles/style_pair.h:16:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/pair_nequip.h:25:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/ATen/ATen.h:7:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Allocator.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Device.h:5:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/Exception.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/StringUtil.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/string_view.h:4:\n",
      "\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:157:65: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1minitializer list cannot be used on the right hand side of operator '>'\u001b[0m\n",
      "          std::tuple_size<std::remove_reference_t<Tuple>>::value>{});\n",
      "\u001b[0;1;32m                                                                ^~~\n",
      "\u001b[0m\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:186:3: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mdeduced return types are a C++14 extension\u001b[0m\n",
      "  decltype(auto) operator()(T&& arg) {\n",
      "\u001b[0;1;32m  ^\n",
      "\u001b[0m\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:217:7: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mno template named 'enable_if_t' in namespace 'std'; did you mean 'fmt::enable_if_t'?\u001b[0m\n",
      "      std::enable_if_t<\n",
      "\u001b[0;1;32m      ^~~~~\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/fmt/core.h:265:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'fmt::enable_if_t' declared here\u001b[0m\n",
      "using enable_if_t = typename std::enable_if<B, T>::type;\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0mIn file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp:22:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/build/styles/style_pair.h:16:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/pair_nequip.h:25:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/ATen/ATen.h:7:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Allocator.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Device.h:5:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/Exception.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/StringUtil.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/string_view.h:4:\n",
      "\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:220:10: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mdeduced return types are a C++14 extension\u001b[0m\n",
      "  static decltype(auto) call(\n",
      "\u001b[0;1;32m         ^\n",
      "\u001b[0m\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:232:7: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mno template named 'enable_if_t' in namespace 'std'; did you mean 'fmt::enable_if_t'?\u001b[0m\n",
      "      std::enable_if_t<\n",
      "\u001b[0;1;32m      ^~~~~\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/fmt/core.h:265:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'fmt::enable_if_t' declared here\u001b[0m\n",
      "using enable_if_t = typename std::enable_if<B, T>::type;\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0mIn file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp:22:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/build/styles/style_pair.h:16:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/pair_nequip.h:25:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/ATen/ATen.h:7:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Allocator.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Device.h:5:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/Exception.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/StringUtil.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/string_view.h:4:\n",
      "\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:235:10: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mdeduced return types are a C++14 extension\u001b[0m\n",
      "  static decltype(auto) call(\n",
      "\u001b[0;1;32m         ^\n",
      "\u001b[0m\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:247:7: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mno template named 'enable_if_t' in namespace 'std'; did you mean 'fmt::enable_if_t'?\u001b[0m\n",
      "      std::enable_if_t<\n",
      "\u001b[0;1;32m      ^~~~~\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/fmt/core.h:265:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'fmt::enable_if_t' declared here\u001b[0m\n",
      "using enable_if_t = typename std::enable_if<B, T>::type;\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0mIn file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp:22:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/build/styles/style_pair.h:16:\n",
      "In file included from /Users/ingrid/PycharmProjects/NequIP/lammps/src/pair_nequip.h:25:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/torch.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:8:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/ATen/ATen.h:7:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Allocator.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/core/Device.h:5:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/Exception.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/StringUtil.h:6:\n",
      "In file included from /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/string_view.h:4:\n",
      "\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:250:10: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mdeduced return types are a C++14 extension\u001b[0m\n",
      "  static decltype(auto) call(\n",
      "\u001b[0;1;32m         ^\n",
      "\u001b[0m\u001b[1m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/c10/util/C++17.h:262:7: \u001b[0m\u001b[0;1;31merror: \u001b[0m\u001b[1mno template named 'enable_if_t' in namespace 'std'; did you mean 'fmt::enable_if_t'?\u001b[0m\n",
      "      std::enable_if_t<\n",
      "\u001b[0;1;32m      ^~~~~\n",
      "\u001b[0m\u001b[1m/Users/ingrid/PycharmProjects/NequIP/lammps/src/fmt/core.h:265:1: \u001b[0m\u001b[0;1;30mnote: \u001b[0m'fmt::enable_if_t' declared here\u001b[0m\n",
      "using enable_if_t = typename std::enable_if<B, T>::type;\n",
      "\u001b[0;1;32m^\n",
      "\u001b[0m\u001b[0;1;31mfatal error: \u001b[0m\u001b[1mtoo many errors emitted, stopping now [-ferror-limit=]\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/hashlittle.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/image.cpp.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/imbalance.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/imbalance_group.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/imbalance_store.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/imbalance_time.cpp.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/imbalance_var.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/improper.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/improper_deprecated.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/improper_hybrid.cpp.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/improper_zero.cpp.o\u001b[0m\n",
      "PLEASE submit a bug report to https://bugs.llvm.org/ and include the crash backtrace, preprocessed source, and associated run script.\n",
      "Stack dump:\n",
      "0.\tProgram arguments: /opt/homebrew/Caskroom/miniforge/base/bin/arm64-apple-darwin20.0.0-clang++ -DLAMMPS_GZIP -DLAMMPS_JPEG -DLAMMPS_MEMALIGN=64 -DLAMMPS_OMP_COMPAT=4 -DLAMMPS_PNG -DLAMMPS_SMALLBIG -DMPICH_SKIP_MPICXX -DOMPI_SKIP_MPICXX -D_MPICC_H -I/Users/ingrid/PycharmProjects/NequIP/lammps/src -I/Users/ingrid/PycharmProjects/NequIP/lammps/build/styles -isystem /opt/homebrew/Cellar/open-mpi/4.1.5/include -isystem /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include -isystem /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -std=c++14 -fmessage-length=0 -isystem /opt/homebrew/Caskroom/miniforge/base/include -arch arm64 -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk -fopenmp=libomp -std=c++11 -MD -MT CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp.o -MF CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp.o.d -o CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp.o -c /Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp\n",
      "1.\t<eof> parser at end of file\n",
      "Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):\n",
      "0  libLLVM-12.dylib         0x000000010cc47334 llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 72\n",
      "1  libLLVM-12.dylib         0x000000010cc461f8 llvm::sys::RunSignalHandlers() + 128\n",
      "2  libLLVM-12.dylib         0x000000010cc46890 llvm::sys::CleanupOnSignal(unsigned long) + 276\n",
      "3  libLLVM-12.dylib         0x000000010cb90cb8 CrashRecoverySignalHandler(int) + 216\n",
      "4  libsystem_platform.dylib 0x0000000191ffb2a4 _sigtramp + 56\n",
      "5  libclang-cpp.12.dylib    0x0000000105a66fec clang::Sema::LookupSpecialMember(clang::CXXRecordDecl*, clang::Sema::CXXSpecialMember, bool, bool, bool, bool, bool) + 108\n",
      "6  libclang-cpp.12.dylib    0x0000000105a67ef4 clang::Sema::LookupDestructor(clang::CXXRecordDecl*) + 36\n",
      "7  libclang-cpp.12.dylib    0x00000001056dff40 clang::StmtVisitorBase<std::__1::add_pointer, (anonymous namespace)::DeferredDiagnosticsEmitter, void>::Visit(clang::Stmt*) + 1412\n",
      "8  libclang-cpp.12.dylib    0x00000001056e0468 clang::EvaluatedExprVisitorBase<std::__1::add_pointer, (anonymous namespace)::DeferredDiagnosticsEmitter>::VisitStmt(clang::Stmt*) + 120\n",
      "9  libclang-cpp.12.dylib    0x00000001056df8bc (anonymous namespace)::DeferredDiagnosticsEmitter::checkFunc(clang::SourceLocation, clang::FunctionDecl*) + 1732\n",
      "10 libclang-cpp.12.dylib    0x00000001056d791c clang::Sema::emitDeferredDiags() + 256\n",
      "11 libclang-cpp.12.dylib    0x00000001056d6dbc clang::Sema::ActOnEndOfTranslationUnitFragment(clang::Sema::TUFragmentKind) + 444\n",
      "12 libclang-cpp.12.dylib    0x00000001056d7b18 clang::Sema::ActOnEndOfTranslationUnit() + 248\n",
      "13 libclang-cpp.12.dylib    0x00000001051283f0 clang::Parser::ParseTopLevelDecl(clang::OpaquePtr<clang::DeclGroupRef>&, bool) + 1120\n",
      "14 libclang-cpp.12.dylib    0x000000010507369c clang::ParseAST(clang::Sema&, bool, bool) + 656\n",
      "15 libclang-cpp.12.dylib    0x00000001065c8720 clang::FrontendAction::Execute() + 124\n",
      "16 libclang-cpp.12.dylib    0x000000010656ad58 clang::CompilerInstance::ExecuteAction(clang::FrontendAction&) + 1848\n",
      "17 libclang-cpp.12.dylib    0x000000010662a090 clang::ExecuteCompilerInvocation(clang::CompilerInstance*) + 1288\n",
      "18 clang-12                 0x000000010249cd9c cc1_main(llvm::ArrayRef<char const*>, char const*, void*) + 1696\n",
      "19 clang-12                 0x000000010249b764 ExecuteCC1Tool(llvm::SmallVectorImpl<char const*>&) + 1032\n",
      "20 libclang-cpp.12.dylib    0x0000000106295c38 void llvm::function_ref<void ()>::callback_fn<clang::driver::CC1Command::Execute(llvm::ArrayRef<llvm::Optional<llvm::StringRef> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, bool*) const::$_1>(long) + 28\n",
      "21 libLLVM-12.dylib         0x000000010cb909c8 llvm::CrashRecoveryContext::RunSafely(llvm::function_ref<void ()>) + 244\n",
      "22 libclang-cpp.12.dylib    0x000000010629545c clang::driver::CC1Command::Execute(llvm::ArrayRef<llvm::Optional<llvm::StringRef> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, bool*) const + 340\n",
      "23 libclang-cpp.12.dylib    0x000000010626f22c clang::driver::Compilation::ExecuteCommand(clang::driver::Command const&, clang::driver::Command const*&) const + 476\n",
      "24 libclang-cpp.12.dylib    0x000000010626f580 clang::driver::Compilation::ExecuteJobs(clang::driver::JobList const&, llvm::SmallVectorImpl<std::__1::pair<int, clang::driver::Command const*> >&) const + 128\n",
      "25 libclang-cpp.12.dylib    0x0000000106281974 clang::driver::Driver::ExecuteCompilation(clang::driver::Compilation&, llvm::SmallVectorImpl<std::__1::pair<int, clang::driver::Command const*> >&) + 216\n",
      "26 clang-12                 0x000000010249af48 main + 9268\n",
      "27 dyld                     0x0000000191ca3e50 start + 2544\n",
      "clang-12: \u001b[0;1;31merror: \u001b[0m\u001b[1mclang frontend command failed with exit code 139 (use -v to see invocation)\u001b[0m\n",
      "clang version 12.0.0\n",
      "Target: aarch64-apple-darwin20.0.0\n",
      "Thread model: posix\n",
      "InstalledDir: /opt/homebrew/Caskroom/miniforge/base/bin\n",
      "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/info.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/integrate.cpp.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/input.cpp.o\u001b[0m\n",
      "clang-12: \u001b[0;1;30mnote: \u001b[0mdiagnostic msg: Error generating preprocessed source(s).\u001b[0m\n",
      "make[2]: *** [CMakeFiles/lammps.dir/build.make:2736: CMakeFiles/lammps.dir/Users/ingrid/PycharmProjects/NequIP/lammps/src/force.cpp.o] Error 139\n",
      "make[2]: *** Waiting for unfinished jobs....\n",
      "make[1]: *** [CMakeFiles/Makefile2:161: CMakeFiles/lammps.dir/all] Error 2\n",
      "make: *** [Makefile:136: all] Error 2\n"
     ]
    }
   ],
   "source": [
    "# compile lammps\n",
    "!git clone -b stable_29Sep2021_update2 --depth 1 https://github.com/lammps/lammps.git\n",
    "!wget \"https://github.com/mir-group/pair_nequip/archive/main.zip\"\n",
    "!unzip -q main.zip\n",
    "!rm main.zip\n",
    "!mv pair_nequip-main pair_nequip\n",
    "!cd pair_nequip && ./patch_lammps.sh ../lammps\n",
    "# !pip install mkl mkl-include\n",
    "!cd lammps && mkdir -p build && cd build && cmake ../cmake -D CMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mkl mkl-include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.utils.cmake_prefix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkl install not working with pip\n",
    "%pip install mkl mkl-include\n",
    "!cd lammps && mkdir -p build && cd build && cmake ../cmake -DCMAKE_PREFIX_PATH=`python -c 'import torch;print(torch.utils.cmake_prefix_path)'` && make -j4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to setup lammps config file?\n",
    "lammps_input_minimize = \"\"\"\n",
    "units\treal\n",
    "atom_style atomic\n",
    "newton off\n",
    "thermo 1\n",
    "read_data structure.data\n",
    "\n",
    "pair_style\tnequip\n",
    "pair_coeff\t* * ../toluene-deployed.pth C H \n",
    "mass            1 15.9994\n",
    "mass            2 1.00794\n",
    "\n",
    "neighbor 1.0 bin\n",
    "neigh_modify delay 5 every 1\n",
    "\n",
    "minimize 0.0 1.0e-8 10000 1000000\n",
    "write_dump all custom output.dump id type x y z fx fy fz\n",
    "\"\"\"\n",
    "!mkdir lammps_run\n",
    "with open(\"lammps_run/toluene_minimize.in\", \"w\") as f:\n",
    "    f.write(lammps_input_minimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toluene_example = \"\"\"15\n",
    " Lattice=\"100.0 0.0 0.0 0.0 100.0 0.0 0.0 0.0 100.0\" Properties=species:S:1:pos:R:3 -169777.5840406276=T pbc=\"F F F\"\n",
    " C       52.48936904      49.86911725      50.09520748\n",
    " C       51.01088202      49.89609925      50.17978049\n",
    " C       50.36647401      50.04650925      48.96054247\n",
    " C       48.95673398      50.29576626      48.71580846\n",
    " C       48.04533296      50.26023426      49.82589448\n",
    " C       48.70932398      49.85770925      51.01923950\n",
    " C       50.06326400      49.77782925      51.25691751\n",
    " H       52.94467905      50.48672926      50.86545150\n",
    " H       52.89060405      48.87175023      50.14480949\n",
    " H       53.02173405      50.05890725      49.03968247\n",
    " H       51.01439802      50.38234726      48.05314045\n",
    " H       48.80598498      50.64314926      47.68195744\n",
    " H       46.96754695      50.20586626      49.53998848\n",
    " H       48.16716997      49.75850325      51.88622952\n",
    " H       50.45791001      49.55387424      52.15303052\n",
    " \"\"\"\n",
    "\n",
    "with open('toluene.xyz', 'w') as f: \n",
    "    f.write(toluene_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read as ASE objects\n",
    "atoms = read('toluene.xyz', format='extxyz')\n",
    "\n",
    "# Perturb positions\n",
    "p = atoms.get_positions()\n",
    "p += np.random.rand(15, 3) * 0.5\n",
    "atoms.set_positions(p)\n",
    "atoms.set_pbc(False)\n",
    "\n",
    "# Write to a LAMMPS file\n",
    "write(\"lammps_run/structure.data\", atoms, format=\"lammps-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ase.io import read\n",
    "# atoms = read('toluene.xyz', index=0)\n",
    "from ase.visualize import view\n",
    "view(atoms, viewer='x3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd lammps_run/ && ../lammps/build/lmp -in toluene_minimize.in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
